{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab5e1194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual category :actual category :actual category :actual category :actual category :actual category :actual category :actual category :actual category :actual category :         footballtennisrugbyfootballcricketfootballathleticstennisfootball\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Train sent0 \n",
      "Train sent0 \n",
      "footballTrain sent0 \n",
      "Train sent0 \n",
      "Train sent0 \n",
      "Train sent0 \n",
      "Train sent0 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Train sent0 \n",
      "Train sent0 \n",
      "\n",
      "\n",
      "Train sent0 \n",
      "\n",
      "Train sent1 \n",
      "\n",
      "Train sent1 \n",
      "\n",
      "Train sent1 \n",
      "\n",
      "Train sent1 \n",
      "\n",
      "Train sent1 \n",
      "\n",
      "Train sent1 \n",
      "\n",
      "Train sent1 \n",
      "\n",
      "Train sent1 \n",
      "\n",
      "Train sent1 \n",
      "\n",
      "Train sent2 \n",
      "\n",
      "Train sent2 \n",
      "\n",
      "Train sent2 \n",
      "\n",
      "Train sent2 \n",
      "\n",
      "Train sent2 \n",
      "\n",
      "Train sent1 \n",
      "\n",
      "Train sent3 \n",
      "\n",
      "Train sent2 \n",
      "\n",
      "Train sent4 \n",
      "\n",
      "Train sent2 \n",
      "\n",
      "Train sent3 \n",
      "\n",
      "Train sent3 \n",
      "\n",
      "Train sent5 \n",
      "\n",
      "Train sent3 \n",
      "\n",
      "Train sent3 \n",
      "\n",
      "Train sent4 \n",
      "\n",
      "Train sent4 \n",
      "\n",
      "Train sent4 \n",
      "\n",
      "Train sent6 \n",
      "\n",
      "Train sent4 \n",
      "\n",
      "Train sent5 \n",
      "\n",
      "Train sent3 \n",
      "\n",
      "Train sent6 \n",
      "\n",
      "Train sent5 \n",
      "\n",
      "Train sent2 \n",
      "\n",
      "Train sent6 \n",
      "\n",
      "Train sent5 \n",
      "\n",
      "Train sent5 \n",
      "Train sent3 \n",
      "\n",
      "\n",
      "Train sent4 \n",
      "\n",
      "Train sent2 \n",
      "\n",
      "Train sent6 \n",
      "\n",
      "Train sent6 \n",
      "\n",
      "Train sent4 \n",
      "\n",
      "Train sent7 \n",
      "\n",
      "Train sent5 \n",
      "\n",
      "Train sent7 \n",
      "\n",
      "Train sent6 \n",
      "\n",
      "Train sent5 \n",
      "\n",
      "Train sent8 \n",
      "\n",
      "Train sent7 \n",
      "\n",
      "Train sent6 \n",
      "\n",
      "Train sent3 \n",
      "\n",
      "Train sent7 \n",
      "\n",
      "Train sent8 \n",
      "\n",
      "Train sent7 \n",
      "\n",
      "Train sent4 \n",
      "\n",
      "Train sent8 \n",
      "\n",
      "Train sent9 \n",
      "\n",
      "Train sent2 \n",
      "\n",
      "Train sent7 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-c6388fcb57b7>:237: OptimizeWarning: Solving system with option 'cholesky':True failed. It is normal for this to happen occasionally, especially as the solution is approached. However, if you see this frequently, consider setting option 'cholesky' to False.\n",
      "  result = linprog(D, A_eq=A_eq[:-1], b_eq=b_eq[:-1]) ## removing redundant to make\n",
      "<ipython-input-6-c6388fcb57b7>:237: OptimizeWarning: Solving system with option 'sym_pos':True failed. It is normal for this to happen occasionally, especially as the solution is approached. However, if you see this frequently, consider setting option 'sym_pos' to False.\n",
      "  result = linprog(D, A_eq=A_eq[:-1], b_eq=b_eq[:-1]) ## removing redundant to make\n",
      "/home2/amit.pandey/miniconda3/lib/python3.9/site-packages/scipy/optimize/_linprog_ip.py:117: LinAlgWarning: Ill-conditioned matrix (rcond=2.9616e-23): result may not be accurate.\n",
      "  return sp.linalg.solve(M, r, sym_pos=sym_pos)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sent3 \n",
      "Train sent8 \n",
      "\n",
      "\n",
      "pred 5 ['football']\n",
      "pred 7 ['football']\n",
      "pred 11 ['football']\n",
      "pred 15 ['football']\n",
      "pred 21 ['football']\n",
      "Train sent8 \n",
      "\n",
      "Train sent5 \n",
      "\n",
      "Train sent7 \n",
      "\n",
      "Train sent9 \n",
      "\n",
      "Train sent8 \n",
      "\n",
      "Train sent4 \n",
      "\n",
      "Train sent9 \n",
      "\n",
      "pred 5 ['football']\n",
      "pred 7 ['athletics']\n",
      "pred 11 ['football']\n",
      "pred 15 ['football']\n",
      "pred 21 ['football']\n",
      "Train sent6 \n",
      "\n",
      "Train sent9 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-c6388fcb57b7>:237: OptimizeWarning: Solving system with option 'cholesky':True failed. It is normal for this to happen occasionally, especially as the solution is approached. However, if you see this frequently, consider setting option 'cholesky' to False.\n",
      "  result = linprog(D, A_eq=A_eq[:-1], b_eq=b_eq[:-1]) ## removing redundant to make\n",
      "<ipython-input-6-c6388fcb57b7>:237: OptimizeWarning: Solving system with option 'sym_pos':True failed. It is normal for this to happen occasionally, especially as the solution is approached. However, if you see this frequently, consider setting option 'sym_pos' to False.\n",
      "  result = linprog(D, A_eq=A_eq[:-1], b_eq=b_eq[:-1]) ## removing redundant to make\n",
      "/home2/amit.pandey/miniconda3/lib/python3.9/site-packages/scipy/optimize/_linprog_ip.py:117: LinAlgWarning: Ill-conditioned matrix (rcond=3.75638e-19): result may not be accurate.\n",
      "  return sp.linalg.solve(M, r, sym_pos=sym_pos)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred 5 ['football']\n",
      "pred 7 ['football']\n",
      "pred 11 ['football']\n",
      "pred 15 ['football']pred 5\n",
      " ['football']pred 21\n",
      " ['football']pred 7\n",
      " ['football']\n",
      "pred 11 ['football']\n",
      "pred 15 ['football']\n",
      "pred 21 ['football']\n",
      "Train sent9 \n",
      "\n",
      "Train sent8 \n",
      "\n",
      "pred 5 ['football']\n",
      "pred 7 ['football']\n",
      "pred 11 ['football']\n",
      "pred 15 ['football']\n",
      "pred 21 ['football']\n",
      "Train sent5 \n",
      "\n",
      "Train sent9 \n",
      "\n",
      "pred 5 ['athletics']\n",
      "pred 7 ['football']\n",
      "pred 11 ['football']\n",
      "pred 15 ['football']\n",
      "pred 21 ['football']\n",
      "Train sent3 \n",
      "\n",
      "Train sent6 \n",
      "\n",
      "Train sent4 \n",
      "\n",
      "Train sent9 \n",
      "\n",
      "Train sent7 \n",
      "\n",
      "Train sent5 \n",
      "\n",
      "pred 5 ['football']\n",
      "pred 7 ['football']\n",
      "pred 11 ['football']\n",
      "pred 15 ['football']\n",
      "pred 21 ['football']\n",
      "Train sent6 \n",
      "\n",
      "Train sent8 \n",
      "\n",
      "Train sent7 \n",
      "\n",
      "Train sent7 \n",
      "\n",
      "Train sent9 \n",
      "\n",
      "Train sent8 \n",
      "\n",
      "pred 5 ['football']\n",
      "pred 7 ['football']\n",
      "pred 11 ['football']\n",
      "pred 15 ['football']\n",
      "pred 21 ['football']\n",
      "Train sent8 \n",
      "\n",
      "Train sent9 \n",
      "\n",
      "pred 5 ['athletics']\n",
      "pred 7 ['athletics']\n",
      "pred 11 ['football']\n",
      "pred 15 ['football']\n",
      "pred 21 ['football']\n",
      "Train sent9 \n",
      "\n",
      "pred 5 ['football']\n",
      "pred 7 ['football']\n",
      "pred 11 ['football']\n",
      "pred 15 ['football']\n",
      "pred 21 ['football']\n",
      "[array([['athletics'],\n",
      "       ['football'],\n",
      "       ['football'],\n",
      "       ['football'],\n",
      "       ['football']], dtype='<U9'), array([['football'],\n",
      "       ['football'],\n",
      "       ['football'],\n",
      "       ['football'],\n",
      "       ['football']], dtype='<U9'), array([['football'],\n",
      "       ['football'],\n",
      "       ['football'],\n",
      "       ['football'],\n",
      "       ['football']], dtype='<U9'), array([['football'],\n",
      "       ['football'],\n",
      "       ['football'],\n",
      "       ['football'],\n",
      "       ['football']], dtype='<U9'), array([['football'],\n",
      "       ['athletics'],\n",
      "       ['football'],\n",
      "       ['football'],\n",
      "       ['football']], dtype='<U9'), array([['athletics'],\n",
      "       ['athletics'],\n",
      "       ['football'],\n",
      "       ['football'],\n",
      "       ['football']], dtype='<U9'), array([['football'],\n",
      "       ['football'],\n",
      "       ['football'],\n",
      "       ['football'],\n",
      "       ['football']], dtype='<U9'), array([['football'],\n",
      "       ['football'],\n",
      "       ['football'],\n",
      "       ['football'],\n",
      "       ['football']], dtype='<U9'), array([['football'],\n",
      "       ['football'],\n",
      "       ['football'],\n",
      "       ['football'],\n",
      "       ['football']], dtype='<U9'), array([['football'],\n",
      "       ['football'],\n",
      "       ['football'],\n",
      "       ['football'],\n",
      "       ['football']], dtype='<U9')]\n",
      "284.2240743637085\n"
     ]
    }
   ],
   "source": [
    "## To run as batch job\n",
    "\n",
    "\n",
    "#imports:\n",
    "\n",
    "# file imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "import os\n",
    "from scipy.optimize import linprog\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from gensim.models import KeyedVectors\n",
    "import sklearn\n",
    "import scipy\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "\n",
    "def sentence_preprocess(embed_dict, sentence,lowercase = 1, strip_punctuation = 1,  remove_stopwords = 1,removedigit = 1):\n",
    "    ''' 1 : True, 0 : False : Lowercase, Strip puncutation, Remove Stopwords, removedigit'''\n",
    "\n",
    "    stop_words = list(stopwords.words('english'))\n",
    "\n",
    "    if lowercase == 1:\n",
    "        sentence = sentence.lower()\n",
    "\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "    if strip_punctuation == 1 and removedigit == 1:\n",
    "        sentence_words = [word for word in sentence_words if word.isalpha()] \n",
    "        \n",
    "\n",
    "\n",
    "    if remove_stopwords == 1:\n",
    "        sentence_words = [word for word in sentence_words if not word in stop_words]\n",
    "    \n",
    "    ## to remove those words which are not in the embeddings that we have.\n",
    "    \n",
    "    sentence_words = [word for word in sentence_words if word in embed_dict.keys()]\n",
    "\n",
    "\n",
    "\n",
    "    return sentence_words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "embeddingtype = None\n",
    "embd_model = None\n",
    "\n",
    "\n",
    "\n",
    "## to load from embedding text files:\n",
    "## have used this to load glove vectors and not word2vec\n",
    "\n",
    "def load_glove(embeddingtype):\n",
    "    \n",
    "    if embeddingtype == 3:\n",
    "        i = 300\n",
    "    if embeddingtype == 4:\n",
    "        i = 200\n",
    "    if embeddingtype == 5:\n",
    "        i = 100\n",
    "    if embeddingtype == 6:\n",
    "        i = 50\n",
    "    \n",
    "    \n",
    "    embeddings_dict = defaultdict(lambda:np.zeros(i)) \n",
    "    # defaultdict to take care of OOV words.\n",
    "    \n",
    "    with open(f\"../files/glove.6B.{i}d.txt\",'r') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "        \n",
    "    return embeddings_dict\n",
    "\n",
    "\n",
    "def embeddings_setup(newembeddingtype):\n",
    "    \n",
    "    \n",
    "    global embeddingtype\n",
    "    global embd_model\n",
    "    \n",
    "    \n",
    "    '''to avoid loading all the embeddings in the memory.'''\n",
    "    \n",
    "    ''''## Note : we are finding the embd matrix two times, ie once for each sentence in\n",
    "        ## the pair of sentences.\n",
    "        ## so this happens that embedding type is changed when find_embmatrix is called\n",
    "        ## by the first sentence.\n",
    "        The above line doesnt matter now as we not calling find_embmatrix , instead we setting up.\n",
    "    '''\n",
    "        \n",
    "        \n",
    "        \n",
    "    if ( embeddingtype != newembeddingtype):\n",
    "        #print(\"embdtype  entered :\", embeddingtype != newembtype,\"\\n\")\n",
    "        #print(\"embd_model type changed to :\", type(embd_model),\"\\n\" )\n",
    "        \n",
    "        embeddingtype = newembeddingtype\n",
    "        \n",
    "        #embd_model = embeddings_setup(embeddingtype) #adictionary\n",
    "        \n",
    "        #print(\"embd_model type changed to :\", type(embd_model),\"\\n\" )\n",
    "        #to make sure that we don't download the embeddings again and again,\n",
    "        # we will check if the embedding type is same as the old one\n",
    "        # and update global embd_model, vrna next time vo use hi nhi ho payega.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if embeddingtype == 1:\n",
    "        embedding = KeyedVectors.load('google300w2v.kv', mmap='r')\n",
    "        ## This will be slower but will prevent kernel from crashing.\n",
    "        \n",
    "        ## comment the above line and uncomment this if you have sufficient RAM:\n",
    "        \n",
    "        #w2v_emb = gensim.downloader.load('word2vec-google-news-300')\n",
    "        \n",
    "    if embeddingtype == 2:\n",
    "        print('Normalised word2vec not loaded, will get it soon')\n",
    "        embedding = None\n",
    "    \n",
    "    if embeddingtype in (3,4,5,6):\n",
    "        embedding = load_glove(embeddingtype)\n",
    "        \n",
    "    \n",
    "    embd_model = embedding\n",
    "    \n",
    "    \n",
    "        \n",
    "def find_embdMatrix(sentence):\n",
    "    global embeddingtype\n",
    "    global embd_model\n",
    "    #print(\" global embedding type being passed is :\", embeddingtype,\"\\n\")\n",
    "    #print(\"embedding type received by the find emb matrix is :\", newembtype,\"\\n\")\n",
    "    #print(\"embd model type is :\", type(embd_model),\"\\n\")\n",
    "    \n",
    "    sent_mtx = []\n",
    "    \n",
    "    \n",
    "    ##commented lines moved to embedding setup.\n",
    "    \n",
    "#     ''''## Note : we are finding the embd matrix two times, ie once for each sentence in\n",
    "#     ## the pair of sentences.\n",
    "#     ## so this happens that embedding type is changed when find_embmatrix is called\n",
    "#     ## by the first sentence\n",
    "#     '''\n",
    "#     if ( embeddingtype != newembtype):\n",
    "#         print(\"if embdtype part entered :\", embeddingtype != newembtype,\"\\n\")\n",
    "        \n",
    "#         embeddingtype = newembtype\n",
    "#         embd_model = embeddings_setup(embeddingtype) #adictionary\n",
    "        \n",
    "#         print(\"embd_model type changed to :\", type(embd_model),\"\\n\" )\n",
    "#     #to make sure that we don't download the embeddings again and again,\n",
    "#     # we will check if the embedding type is same as the old one\n",
    "#     # and update global embd_model, vrna next time vo use hi nhi ho payega.\n",
    "    \n",
    "    #print(\"embd_model type changed to :\", type(embd_model),\"\\n\" )\n",
    "    for word in sentence:\n",
    "        word_emb = embd_model[word]\n",
    "        sent_mtx.append(word_emb)\n",
    "    \n",
    "    sent_mtx = np.array(sent_mtx).reshape(len(sentence),-1)\n",
    "\n",
    "    return sent_mtx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def wasserstein_distance(pi, qj, D, cost = 'min'):\n",
    "        \"\"\"Find Wasserstein distance through linear programming\n",
    "        p.shape=[m], q.shape=[n], D.shape=[m, n]\n",
    "    \n",
    "        suppose doc1 has m words and doc2 has n words, then an mxn array would be formed, \n",
    "        having distance of each word in doc1 to that of doc2.\n",
    "    \n",
    "    \n",
    "    \n",
    "        p.sum()=1, q.sum()=1, p∈[0,1], q∈[0,1]\n",
    "        \"\"\"\n",
    "        A_eq = [] # a list which will later be converted to array after appending.\n",
    "        for i in range(len(pi)): # len = number of words.\n",
    "            A = np.zeros_like(D) # a 2d array made with the shape of D.  \n",
    "            A[i, :] = 1 \n",
    "            #print(\"Dshape, len pi till here :\",D.shape,len(pi),\"\\n\")\n",
    "            \n",
    "            # to make summation over \"i\" of Tij = pi, ie total / sum of outflow\n",
    "            ## from one word is equal to its pi (normalized bag of word/ frequency/density)\n",
    "            ## ex : if 2x3 D:\n",
    "            ##T1,1 + T1,2 + T1,3 + 0 T2,1 + 0 T2,2 + 0 T2,3 = P1 and so on for every i,\n",
    "            ## ie for each word in the doc1\n",
    "            \n",
    "            \n",
    "            #print(\"A.shape\", A.shape,\"\\n\")\n",
    "            A_eq.append(A.reshape(-1)) ## reshape(-1) flatens and then appending in A_eq.\n",
    "            \n",
    "            #print(A_eq,\"Aeq\\n\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            ## A_eq will be (m+n)x(m.n)\n",
    "    \n",
    "        for i in range(len(qj)):\n",
    "            A = np.zeros_like(D)\n",
    "            A[:, i] = 1 ## summation over \"j\" this time, so this time for different rows, \n",
    "            ## over a column \"j\" which refers to doc2, ie total incoming flow = qj density\n",
    "            A_eq = list(A_eq)\n",
    "            A_eq.append(A.reshape(-1))\n",
    "            A_eq = np.array(A_eq)\n",
    "        \n",
    "        #print(A_eq.shape,A_eq)\n",
    "       \n",
    "        b_eq = np.concatenate([pi, qj])\n",
    "        D = D.reshape(-1)\n",
    "        #print(\"Dshape:\",D.shape)\n",
    "        if cost == 'max':\n",
    "            D = D*(-1)\n",
    "        \n",
    "        result = linprog(D, A_eq=A_eq[:-1], b_eq=b_eq[:-1]) ## removing redundant to make \n",
    "        ## solution more robust.\n",
    "        return np.absolute(result.fun), result.x , D.reshape((len(pi),len(qj)))  ## fun returns the final optimized value, x returns each value of xi,j that is the array\n",
    "\n",
    "    \n",
    "def relaxed_distance(pi,qj,D,cost='min'):\n",
    "    \n",
    "    # to find relaxed we just add the min/max cost directly using the least distance for pi to qj.\n",
    "    \n",
    "    # D is calculated from P to Q ie P in rows and Q in columns, To find Q to P we will transpose \n",
    "    if cost == 'min':\n",
    "        p_to_q = np.dot(D.min(axis=1),pi)\n",
    "        q_to_p = np.dot(D.T.min(axis=1),qj)\n",
    "        \n",
    "        return max(p_to_q,q_to_p)\n",
    "    \n",
    "    if cost == 'max':\n",
    "        \n",
    "        p_to_q = np.dot(D.max(axis=1),pi)\n",
    "        q_to_p = np.dot(D.T.max(axis=1),qj)\n",
    "        \n",
    "        return min(p_to_q,q_to_p), None, D\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "class WMD:\n",
    "    \n",
    "    ''' wmd type = normal/relaxed, costtype = min/max.\n",
    "    Enter Two sentence strings, cost = max if you want to try \n",
    "    max cost max flow version, embeddingtype = 1 for word2vec, 2 = normalized\n",
    "    word2vec, 3 = glove300d, 4 = glove200d, 5 = glove100d 6 = glove50d'''\n",
    "    \n",
    "    def __init__(self,embeddingtype, wmd_type = 'normal', costtype='min'):\n",
    "        \n",
    "        \n",
    "        self.cost = costtype\n",
    "        \n",
    "        self.embeddingtype = embeddingtype \n",
    "        self.wmd_type = wmd_type\n",
    "        \n",
    "        \n",
    "        ## setting up the embeddings\n",
    "        \n",
    "        embeddings_setup(self.embeddingtype)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    #def word_count(self):\n",
    "#         self.sent1_dic = defaultdict(int)\n",
    "#         self.sent2_dic = defaultdict(int)\n",
    "        \n",
    "#         for word in sorted(sentence_preprocess(self.sent1)):\n",
    "#             self.sent1_dic[word] += 1\n",
    "            \n",
    "#         for word in sorted(sentence_preprocess(self.sent2)):\n",
    "#             self.sent2_dic[word] += 1\n",
    "        \n",
    "#         return dict(self.sent1_dic), dict(self.sent2_dic)\n",
    "\n",
    "\n",
    "\n",
    "#     def wasserstein_distance(self, pi, qj, D):\n",
    "#         \"\"\"Find Wasserstein distance through linear programming\n",
    "#         p.shape=[m], q.shape=[n], D.shape=[m, n]\n",
    "    \n",
    "#         suppose doc1 has m words and doc2 has n words, then an mxn array would be formed, \n",
    "#         having distance of each word in doc1 to that of doc2.\n",
    "    \n",
    "    \n",
    "    \n",
    "#         p.sum()=1, q.sum()=1, p∈[0,1], q∈[0,1]\n",
    "#         \"\"\"\n",
    "#         A_eq = [] # a list which will later be converted to array after appending.\n",
    "#         for i in range(len(pi)): # len = number of words.\n",
    "#             A = np.zeros_like(D) # a 2d array made with the shape of D.  \n",
    "#             A[i, :] = 1 \n",
    "#             # to make summation over \"i\" of Tij = pi, ie total / sum of outflow\n",
    "            ## from one word is equal to its pi (normalized bag of word/ frequency/density)\n",
    "            ## ex : if 2x3 D:\n",
    "            ##T1,1 + T1,2 + T1,3 + 0 T2,1 + 0 T2,2 + 0 T2,3 = P1 and so on for every i,\n",
    "            ## ie for each word in the doc1\n",
    "        \n",
    "#             A_eq.append(A.reshape(-1)) ## reshape(-1) flatens and then appending in A_eq.\n",
    "            ## A_eq will be (m+n)x(m.n)\n",
    "    \n",
    "#         for i in range(len(qj)):\n",
    "#             A = np.zeros_like(D)\n",
    "#             A[:, i] = 1 ## summation over \"j\" this time, so this time for different rows, \n",
    "#             ## over a column \"j\" which refers to doc2, ie total incoming flow = qj density\n",
    "#             A_eq.append(A.reshape(-1))\n",
    "#             A_eq = np.array(A_eq)\n",
    "        \n",
    "#         print(A_eq.shape,A_eq)\n",
    "       \n",
    "#         b_eq = np.concatenate([pi, qj])\n",
    "#         D = D.reshape(-1)\n",
    "#         if self.cost == 'max':\n",
    "#             D = D*(-1)\n",
    "        \n",
    "#         result = linprog(D, A_eq=A_eq[:-1], b_eq=b_eq[:-1]) ## removing redundant to make \n",
    "#         ## solution more robust.\n",
    "#         return result.fun, result.x  ## fun returns the final optimized value, x returns each value of xi,j that is the array\n",
    "\n",
    "    \n",
    "    def word_mover_distance(self,sentence1,sentence2):\n",
    "        \n",
    "        self.sent1 = sentence1\n",
    "        #print(self.sent1 ,\"\\n\")\n",
    "        self.sent2 = sentence2\n",
    "        #print(self.sent2 ,\"\\n\")\n",
    "        \n",
    "        \n",
    "        self.sent1_dic = defaultdict(int)\n",
    "        self.sent2_dic = defaultdict(int)\n",
    "        \n",
    "        for word in sorted(sentence_preprocess(embd_model,self.sent1)): # sorted to have better\n",
    "            self.sent1_dic[word] += 1 # idea of the sequence of the words. Creating BOW here\n",
    "            \n",
    "        for word in sorted(sentence_preprocess(embd_model,self.sent2)): #creating BOW from sorted sequence\n",
    "            self.sent2_dic[word] += 1\n",
    "        \n",
    "        \n",
    "        self.sent1_dic = dict(self.sent1_dic) # converted from default dict to dict.\n",
    "        self.sent2_dic = dict(self.sent2_dic) # because following operations work on dict\n",
    "        \n",
    "        \n",
    "        #print(self.sent1_dic ,\"\\n\")\n",
    "        #print(self.sent2_dic ,\"\\n\")\n",
    "        \n",
    "        \n",
    "        ## Now we will store a list/array of all the words in each sentence (in alphabetically sorted order)\n",
    "        ## we will store corresponding count, and then corresponding Normalised count.\n",
    "        self.sent1_words = np.array(list(self.sent1_dic.keys())) #dictionary keys converted to list than array\n",
    "        self.sent1_counts = np.array(list(self.sent1_dic.values()))\n",
    "        \n",
    "        self.sent2_words = np.array(list(self.sent2_dic.keys()))\n",
    "        self.sent2_counts = np.array(list(self.sent2_dic.values()))\n",
    "        \n",
    "        \n",
    "        #print(self.sent1_words ,\"\\n\")\n",
    "        #print(self.sent1_counts ,\"\\n\")\n",
    "        \n",
    "        #print(self.sent2_words ,\"\\n\")\n",
    "        #print(self.sent2_counts ,\"\\n\")\n",
    "        \n",
    "        #dictionary values cant be converted into an array directly, hence the\n",
    "        #list step.\n",
    "        \n",
    "        #print(\"embedding type being passed is :\", self.embeddingtype,\"\\n\")\n",
    "        self.sent1_embmtx = find_embdMatrix(self.sent1_words)\n",
    "        #print(self.sent1_embmtx.shape,\"sent1emb\\n\")\n",
    "        self.sent2_embmtx = find_embdMatrix(self.sent2_words)\n",
    "        #print(self.sent2_embmtx.shape,\"sent2emb\\n\")\n",
    "        \n",
    "        self.pi = self.sent1_counts/np.sum(self.sent1_counts) #NBOW step from BOW\n",
    "        #print(self.pi,\"self.pi\\n\")\n",
    "        self.qj = self.sent2_counts/np.sum(self.sent2_counts)\n",
    "        #print(self.qj,\"self.qj\\n\")\n",
    "        \n",
    "        self.D = np.sqrt(np.square(self.sent1_embmtx[:, None] - self.sent2_embmtx[None, :]).sum(axis=2)) \n",
    "        #print(self.D.shape,\"Dshape \\n\")\n",
    "        ## programmers sought used mean instead of sum.\n",
    "        ## scipy cdist can be used as well.\n",
    "        \n",
    "        if self.wmd_type == 'normal':\n",
    "            return wasserstein_distance(self.pi, self.qj, self.D, self.cost)\n",
    "        \n",
    "        \n",
    "        if self.wmd_type == 'relaxed':\n",
    "            return relaxed_distance(self.pi,self.qj,self.D,self.cost)\n",
    "\n",
    "\n",
    "## KNN\n",
    "\n",
    "Train_BBCsport_sent = np.load(\"../files/Train_BBCsport_sent.npy\")\n",
    "Train_BBCsport_label = np.load(\"../files/Train_BBCsport_label.npy\")\n",
    "Test_BBCsport_sent = np.load(\"../files/Test_BBCsport_sent.npy\")\n",
    "Test_BBCsport_label = np.load(\"../files/Test_BBCsport_label.npy\")\n",
    "\n",
    "\n",
    "embeddingtype = 3\n",
    "model = WMD(embeddingtype,wmd_type = 'relaxed', costtype='max')\n",
    "\n",
    "            \n",
    "    \n",
    "no_testdocs = len(Test_BBCsport_sent)\n",
    "no_testlabels = len(Test_BBCsport_label)\n",
    "#no_testdocs,no_testlabels\n",
    "\n",
    "\n",
    "\n",
    "actual_category = []\n",
    "predicted_category = []\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "embeddingtype = 3\n",
    "model = WMD(embeddingtype,wmd_type = 'normal', costtype='min')\n",
    "\n",
    "\n",
    "def predict_Category(i):\n",
    "    sentence = Test_BBCsport_sent[i]\n",
    "    print(\"actual category :\", Test_BBCsport_label[i])\n",
    "    actual_category.append(Test_BBCsport_label[i])\n",
    "    distance_fromTrainset = []\n",
    "    \n",
    "    #for i in range (len(Train)):\n",
    "       \n",
    "    for i in range (3):\n",
    "        print(f\"Train sent{i} \\n\")\n",
    "        ## Find totalcost ie distance between sentence passed from test set to each sentence \n",
    "        ## in training set. and then append in the list.\n",
    "        \n",
    "        #print(sentence)\n",
    "        #print(Train_BBCsport_sent[i])\n",
    "        \n",
    "        Totalcost, Tcoeff, Distancematx = model.word_mover_distance(sentence,Train_BBCsport_sent[i])\n",
    "        #print(Totalcost)\n",
    "        distance_fromTrainset.append(Totalcost)\n",
    "        \n",
    "    distance_fromTrainset = np.array(distance_fromTrainset)\n",
    "    #print('distance from train set array:',distance_fromTrainset)\n",
    "    \n",
    "    arr1indx = distance_fromTrainset.argsort()\n",
    "    \n",
    "    \n",
    "    ## Taking for different values of K\n",
    "    \n",
    "    #k = 5\n",
    "    sorted_distance_fromTrainset_k5 = distance_fromTrainset[arr1indx[::1]][:5]\n",
    "    sorted_labels_k5 = Train_BBCsport_label[arr1indx[::1]][:5]\n",
    "    \n",
    "    predicted_cat_k5 = scipy.stats.mode(sorted_labels_k5)[0]\n",
    "    print(\"pred 5\",predicted_cat_k5)\n",
    "   \n",
    "\n",
    "    #k = 7\n",
    "    sorted_distance_fromTrainset_k7 = distance_fromTrainset[arr1indx[::1]][:7]\n",
    "    sorted_labels_k7 = Train_BBCsport_label[arr1indx[::1]][:7]\n",
    "    \n",
    "    predicted_cat_k7 = scipy.stats.mode(sorted_labels_k7)[0]\n",
    "    print(\"pred 7\",predicted_cat_k7)\n",
    "\n",
    "    #k = 11\n",
    "    sorted_distance_fromTrainset_k11 = distance_fromTrainset[arr1indx[::1]][:11]\n",
    "    sorted_labels_k11 = Train_BBCsport_label[arr1indx[::1]][:11]\n",
    "    \n",
    "    predicted_cat_k11 = scipy.stats.mode(sorted_labels_k11)[0]\n",
    "    print(\"pred 11\",predicted_cat_k11)\n",
    "\n",
    "    #k = 15\n",
    "    sorted_distance_fromTrainset_k15 = distance_fromTrainset[arr1indx[::1]][:15]\n",
    "    sorted_labels_k15 = Train_BBCsport_label[arr1indx[::1]][:15]\n",
    "    \n",
    "    predicted_cat_k15 = scipy.stats.mode(sorted_labels_k15)[0]\n",
    "    print(\"pred 15\",predicted_cat_k15)\n",
    "\n",
    "    #k = 21\n",
    "    sorted_distance_fromTrainset_k21 = distance_fromTrainset[arr1indx[::1]][:21]\n",
    "    sorted_labels_k21 = Train_BBCsport_label[arr1indx[::1]][:21]\n",
    "    \n",
    "    predicted_cat_k21 = scipy.stats.mode(sorted_labels_k21)[0]\n",
    "    print(\"pred 21\",predicted_cat_k21)\n",
    "\n",
    "\n",
    "    #print(sorted_distance_fromTrainset,sorted_labels)\n",
    "\n",
    "    \n",
    "    return Test_BBCsport_label[i] ,np.array([predicted_cat_k5,predicted_cat_k7,predicted_cat_k11,predicted_cat_k15,predicted_cat_k21])   \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "no_testdocs = len(Test_BBCsport_sent)\n",
    "no_testlabels = len(Test_BBCsport_label)\n",
    "#no_testdocs,no_testlabels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#predicted_categories_list = []\n",
    "# for i in range (1,2):\n",
    "#     print(Test_BBCsport_label[i])\n",
    "#     actual_categories.append(Test_BBCsport_label[i]) \n",
    "#     pred_category = predict_Category(Test_BBCsport_sent[i])\n",
    "#     print(pred_category)\n",
    "#     predicted_categories_list.append(pred_category)\n",
    "\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "with Pool(10) as p :\n",
    "        actual_categoriesList, predicted_Categorieslist = p.map(predict_Category,[0,1,2,3,4,5,6,7,8,9])\n",
    "        \n",
    "print(predicted_Categorieslist)\n",
    "\n",
    "\n",
    "et = time.time()\n",
    "\n",
    "print(et-st)\n",
    "\n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "   \n",
    " \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc631248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['athletics']\n",
      " ['football']\n",
      " ['football']\n",
      " ['football']\n",
      " ['football']]\n",
      "[['football']\n",
      " ['football']\n",
      " ['football']\n",
      " ['football']\n",
      " ['football']]\n",
      "[['football']\n",
      " ['football']\n",
      " ['football']\n",
      " ['football']\n",
      " ['football']]\n",
      "[['football']\n",
      " ['football']\n",
      " ['football']\n",
      " ['football']\n",
      " ['football']]\n",
      "[['football']\n",
      " ['athletics']\n",
      " ['football']\n",
      " ['football']\n",
      " ['football']]\n",
      "[['athletics']\n",
      " ['athletics']\n",
      " ['football']\n",
      " ['football']\n",
      " ['football']]\n",
      "[['football']\n",
      " ['football']\n",
      " ['football']\n",
      " ['football']\n",
      " ['football']]\n",
      "[['football']\n",
      " ['football']\n",
      " ['football']\n",
      " ['football']\n",
      " ['football']]\n",
      "[['football']\n",
      " ['football']\n",
      " ['football']\n",
      " ['football']\n",
      " ['football']]\n",
      "[['football']\n",
      " ['football']\n",
      " ['football']\n",
      " ['football']\n",
      " ['football']]\n"
     ]
    }
   ],
   "source": [
    "for i in range (10):\n",
    "    \n",
    "    print(predicted_Categorieslist[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a42f352d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(actual_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b23b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = []\n",
    "\n",
    "def fun(i):\n",
    "    aa.append(i)\n",
    "    return i,np.arange((i**i))\n",
    "\n",
    "with Pool(3) as p:\n",
    "      xx = p.map(fun,[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60eadb27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, array([0]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68c7c4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81e99a55",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_save_dispatcher() got an unexpected keyword argument 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a0d28ed5266b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.npy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: _save_dispatcher() got an unexpected keyword argument 'dtype'"
     ]
    }
   ],
   "source": [
    "np.save((\"test.npy\",xx),dtype = object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c373781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
