{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d420a1ff",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code started and imports done \n",
      "\n",
      "\n",
      " Directory check :\n",
      "\n",
      "\n",
      " /scratch/Amit_Pandey/wmd_lite/files: ['bbcsport_labels.npy', 'Train_BBCsport_sent.npy', 'bbcsport_dataset.json', 'data.json', 'Test_BBCsport_label.npy', 'bbcsport_sentences.npy', 'glove.6B.zip', 'glove.6B.50d.txt', 'result_Dhawal.pickle', 'model.npy', 'reduced_glove_embedding_300.json', 'model.npz', 'google300w2v.kv', 'glove.6B.200d.txt', 'glove.6B.100d.txt', 'wmd_exp_v4_for_loop_RE_for_Dhawal.ipynb', 'glove.6B.300d.txt', 'bbcsport', 'Train_BBCsport_label.npy', 'Test_BBCsport_sent.npy']\n",
      " \n",
      " os.listdir /scratch/Amit_Pandey/gensim-data/word2vec-google-news-300/: ['word2vec-google-news-300.gz', 'GoogleNews-vectors-negative300.bin', '__pycache__', '__init__.py']\n",
      "\n",
      " printing current nltk path and adding to the path:\n",
      "['/home2/dhawals1939/nltk_data', '/home2/dhawals1939/miniconda3/nltk_data', '/home2/dhawals1939/miniconda3/share/nltk_data', '/home2/dhawals1939/miniconda3/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '\\n /scratch/Amit_Pandey/nltk_data']\n",
      "\n",
      " ['/home2/dhawals1939/nltk_data', '/home2/dhawals1939/miniconda3/nltk_data', '/home2/dhawals1939/miniconda3/share/nltk_data', '/home2/dhawals1939/miniconda3/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '\\n /scratch/Amit_Pandey/nltk_data', '/scratch/Amit_Pandey/nltk_data']\n",
      "\n",
      " FUNCTIONS DEFINITION OVER AND DATA LOADING STARTED\n",
      "\n",
      "\n",
      " DATA LOADING ENDED\n",
      "\n",
      "##################Train details:\n",
      "\n",
      "\n",
      " MODEL INITIALIZATION STARTED\n",
      "\n",
      "12392\n",
      "\n",
      " MODEL INITIALIZATION OVER\n",
      "\n",
      " \n",
      " ################### Test50 ############# \n",
      "\n",
      "\n",
      "Train0\n",
      " distance btwn test50 and train0 : 6.768940438075009 \n",
      "\n",
      "\n",
      "Train1\n",
      " distance btwn test50 and train1 : 6.69551759482227 \n",
      "\n",
      "\n",
      "Train2\n",
      " distance btwn test50 and train2 : 6.7041507207270765 \n",
      "\n",
      "\n",
      "Train3\n",
      " distance btwn test50 and train3 : 6.042794495978872 \n",
      "\n",
      "\n",
      "Train4\n",
      " distance btwn test50 and train4 : 6.114087225675572 \n",
      "\n",
      "\n",
      "Train5\n",
      " distance btwn test50 and train5 : 6.3291091999106825 \n",
      "\n",
      "\n",
      "Train6\n",
      " distance btwn test50 and train6 : 5.757876794123235 \n",
      "\n",
      "\n",
      "Train7\n",
      " distance btwn test50 and train7 : 5.946441557872616 \n",
      "\n",
      "\n",
      "Train8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12428/548447628.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m220\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# number of test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\" \\n ################### Test{i} #############\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mpredict_Category\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;31m#actual_categories.append(Test_BBCsport_label[i])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12428/548447628.py\u001b[0m in \u001b[0;36mpredict_Category\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nTrain{j}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0mTotalcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTcoeff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistancematx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_mover_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTrain_BBCsport_sent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\" distance btwn test{i} and train{j} :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTotalcost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;31m#print(Totalcost)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12428/548447628.py\u001b[0m in \u001b[0;36mword_mover_distance\u001b[0;34m(self, sentence1, sentence2)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwmd_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'normal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwasserstein_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12428/548447628.py\u001b[0m in \u001b[0;36mwasserstein_distance\u001b[0;34m(pi, qj, D, cost)\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinprog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_eq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mA_eq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_eq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb_eq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## removing redundant to make\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0;31m## solution more robust.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## fun returns the final optimized value, x returns each value of xi,j that is the array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/scipy/optimize/_linprog.py\u001b[0m in \u001b[0;36mlinprog\u001b[0;34m(c, A_ub, b_ub, A_eq, b_eq, bounds, method, callback, options, x0)\u001b[0m\n\u001b[1;32m    639\u001b[0m                 postsolve_args=postsolve_args, **solver_options)\n\u001b[1;32m    640\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'interior-point'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m             x, status, message, iteration = _linprog_ip(\n\u001b[0m\u001b[1;32m    642\u001b[0m                 \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m                 postsolve_args=postsolve_args, **solver_options)\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/scipy/optimize/_linprog_ip.py\u001b[0m in \u001b[0;36m_linprog_ip\u001b[0;34m(c, c0, A, b, callback, postsolve_args, maxiter, tol, disp, alpha0, beta, sparse, lstsq, sym_pos, cholesky, pc, ip, permc_spec, **unknown_options)\u001b[0m\n\u001b[1;32m   1117\u001b[0m     \u001b[0mcholesky\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcholesky\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcholesky\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msym_pos\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlstsq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m     x, status, message, iteration = _ip_hsd(A, b, c, c0, alpha0, beta,\n\u001b[0m\u001b[1;32m   1120\u001b[0m                                             \u001b[0mmaxiter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m                                             \u001b[0mlstsq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msym_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcholesky\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/scipy/optimize/_linprog_ip.py\u001b[0m in \u001b[0;36m_ip_hsd\u001b[0;34m(A, b, c, c0, alpha0, beta, maxiter, disp, tol, sparse, lstsq, sym_pos, cholesky, pc, ip, permc_spec, callback, postsolve_args)\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;31m# Solve [4] 8.6 and 8.7/8.13/8.23\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m             d_x, d_y, d_z, d_tau, d_kappa = _get_delta(\n\u001b[0m\u001b[1;32m    751\u001b[0m                 \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkappa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m                 sparse, lstsq, sym_pos, cholesky, pc, ip, permc_spec)\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/scipy/optimize/_linprog_ip.py\u001b[0m in \u001b[0;36m_get_delta\u001b[0;34m(A, b, c, x, y, z, tau, kappa, gamma, eta, sparse, lstsq, sym_pos, cholesky, pc, ip, permc_spec)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDinv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDinv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0msolve\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_solver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstsq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msym_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcholesky\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermc_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## To run as batch job\n",
    "\n",
    "\n",
    "#imports:\n",
    "\n",
    "# file imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "import os\n",
    "from scipy.optimize import linprog\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from gensim.models import KeyedVectors\n",
    "import sklearn\n",
    "import scipy\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import pickle\n",
    "from gensim import models\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "print(\"code started and imports done \\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n Directory check :\\n\")\n",
    "print(\"\\n /scratch/Amit_Pandey/wmd_lite/files:\", os.listdir(\"/scratch/Amit_Pandey/wmd_lite/files\"))\n",
    "print(\" \\n os.listdir /scratch/Amit_Pandey/gensim-data/word2vec-google-news-300/:\",\n",
    "      os.listdir(\"/scratch/Amit_Pandey/gensim-data/word2vec-google-news-300/\"))\n",
    "\n",
    "print(\"\\n printing current nltk path and adding to the path:\")\n",
    "print(nltk.data.path)\n",
    "\n",
    "nltk.data.path.append(\"/scratch/Amit_Pandey/nltk_data\")\n",
    "\n",
    "print(\"\\n\",nltk.data.path) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sentence_preprocess(embed_dict, sentence,lowercase = 1, strip_punctuation = 1,  remove_stopwords = 1,removedigit = 1):\n",
    "    ''' 1 : True, 0 : False : Lowercase, Strip puncutation, Remove Stopwords, removedigit'''\n",
    "\n",
    "    stop_words = list(stopwords.words('english'))\n",
    "\n",
    "    if lowercase == 1:\n",
    "        sentence = sentence.lower()\n",
    "\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "    if strip_punctuation == 1 and removedigit == 1:\n",
    "        sentence_words = [word for word in sentence_words if word.isalpha()] \n",
    "        \n",
    "\n",
    "\n",
    "    if remove_stopwords == 1:\n",
    "        sentence_words = [word for word in sentence_words if not word in stop_words]\n",
    "    \n",
    "    ## to remove those words which are not in the embeddings that we have.\n",
    "    \n",
    "    sentence_words = [word for word in sentence_words if word in embed_dict.keys()]\n",
    "\n",
    "\n",
    "\n",
    "    return sentence_words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "embeddingtype = None\n",
    "embd_model = None\n",
    "\n",
    "\n",
    "\n",
    "## to load from embedding text files:\n",
    "## have used this to load glove vectors and not word2vec\n",
    "\n",
    "def load_glove(embeddingtype):\n",
    "    \n",
    "    if embeddingtype == 3:\n",
    "        i = 300\n",
    "        \n",
    "        a_file = open(\"/scratch/Amit_Pandey/wmd_lite/files/reduced_glove_embedding_300.json\", \"r\")\n",
    "        output = json.load(a_file)\n",
    "        print(len(output.keys()))\n",
    "        a_file.close()\n",
    "        \n",
    "        embeddings_dict = output\n",
    "        \n",
    "    if embeddingtype == 4:\n",
    "        i = 200\n",
    "    if embeddingtype == 5:\n",
    "        i = 100\n",
    "    if embeddingtype == 6:\n",
    "        i = 50\n",
    "    \n",
    "    \n",
    "#     embeddings_dict = defaultdict(lambda:np.zeros(i)) \n",
    "#     # defaultdict to take care of OOV words.\n",
    "    \n",
    "#     with open(f\"../files/glove.6B.{i}d.txt\",'r') as f:\n",
    "#         for line in f:\n",
    "#             values = line.split()\n",
    "#             word = values[0]\n",
    "#             vector = np.asarray(values[1:], \"float32\")\n",
    "#             embeddings_dict[word] = vector\n",
    "        \n",
    "    return embeddings_dict\n",
    "\n",
    "\n",
    "\n",
    "def embeddings_setup(newembeddingtype):\n",
    "    \n",
    "    \n",
    "    global embeddingtype\n",
    "    global embd_model\n",
    "    \n",
    "    \n",
    "    '''to avoid loading all the embeddings in the memory.'''\n",
    "    \n",
    "    ''''## Note : we are finding the embd matrix two times, ie once for each sentence in\n",
    "        ## the pair of sentences.\n",
    "        ## so this happens that embedding type is changed when find_embmatrix is called\n",
    "        ## by the first sentence.\n",
    "        The above line doesnt matter now as we not calling find_embmatrix , instead we setting up.\n",
    "    '''\n",
    "        \n",
    "        \n",
    "        \n",
    "    if ( embeddingtype != newembeddingtype):\n",
    "        #print(\"embdtype  entered :\", embeddingtype != newembtype,\"\\n\")\n",
    "        #print(\"embd_model type changed to :\", type(embd_model),\"\\n\" )\n",
    "        \n",
    "        embeddingtype = newembeddingtype\n",
    "        \n",
    "        #embd_model = embeddings_setup(embeddingtype) #adictionary\n",
    "        \n",
    "        #print(\"embd_model type changed to :\", type(embd_model),\"\\n\" )\n",
    "        #to make sure that we don't download the embeddings again and again,\n",
    "        # we will check if the embedding type is same as the old one\n",
    "        # and update global embd_model, vrna next time vo use hi nhi ho payega.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if embeddingtype == 1:\n",
    "        \n",
    "        ## To load from scratch:\n",
    "        \n",
    "        w = models.KeyedVectors.load_word2vec_format(\n",
    "        '/scratch/Amit_Pandey/gensim-data/word2vec-google-news-300/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "        \n",
    "        embedding = w\n",
    "        \n",
    "        #embedding = KeyedVectors.load('google300w2v.kv', mmap='r')\n",
    "        ## This will be slower but will prevent kernel from crashing.\n",
    "        \n",
    "        ## comment the above line and uncomment this if you have sufficient RAM:\n",
    "        \n",
    "        #w2v_emb = gensim.downloader.load('word2vec-google-news-300')\n",
    "        \n",
    "    if embeddingtype == 2:\n",
    "        print('Normalised word2vec not loaded, will get it soon')\n",
    "        embedding = None\n",
    "    \n",
    "    if embeddingtype in (3,4,5,6):\n",
    "        embedding = load_glove(embeddingtype)\n",
    "        \n",
    "    \n",
    "    embd_model = embedding\n",
    "    \n",
    "    \n",
    "        \n",
    "def find_embdMatrix(sentence):\n",
    "    global embeddingtype\n",
    "    global embd_model\n",
    "    #print(\" global embedding type being passed is :\", embeddingtype,\"\\n\")\n",
    "    #print(\"embedding type received by the find emb matrix is :\", newembtype,\"\\n\")\n",
    "    #print(\"embd model type is :\", type(embd_model),\"\\n\")\n",
    "    \n",
    "    sent_mtx = []\n",
    "    \n",
    "    \n",
    "    ##commented lines moved to embedding setup.\n",
    "    \n",
    "#     ''''## Note : we are finding the embd matrix two times, ie once for each sentence in\n",
    "#     ## the pair of sentences.\n",
    "#     ## so this happens that embedding type is changed when find_embmatrix is called\n",
    "#     ## by the first sentence\n",
    "#     '''\n",
    "#     if ( embeddingtype != newembtype):\n",
    "#         print(\"if embdtype part entered :\", embeddingtype != newembtype,\"\\n\")\n",
    "        \n",
    "#         embeddingtype = newembtype\n",
    "#         embd_model = embeddings_setup(embeddingtype) #adictionary\n",
    "        \n",
    "#         print(\"embd_model type changed to :\", type(embd_model),\"\\n\" )\n",
    "#     #to make sure that we don't download the embeddings again and again,\n",
    "#     # we will check if the embedding type is same as the old one\n",
    "#     # and update global embd_model, vrna next time vo use hi nhi ho payega.\n",
    "    \n",
    "    #print(\"embd_model type changed to :\", type(embd_model),\"\\n\" )\n",
    "    for word in sentence:\n",
    "        word_emb = embd_model[word]\n",
    "        sent_mtx.append(word_emb)\n",
    "    \n",
    "    sent_mtx = np.array(sent_mtx).reshape(len(sentence),-1)\n",
    "\n",
    "    return sent_mtx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def wasserstein_distance(pi, qj, D, cost = 'min'):\n",
    "        \"\"\"Find Wasserstein distance through linear programming\n",
    "        p.shape=[m], q.shape=[n], D.shape=[m, n]\n",
    "    \n",
    "        suppose doc1 has m words and doc2 has n words, then an mxn array would be formed, \n",
    "        having distance of each word in doc1 to that of doc2.\n",
    "    \n",
    "    \n",
    "    \n",
    "        p.sum()=1, q.sum()=1, p∈[0,1], q∈[0,1]\n",
    "        \"\"\"\n",
    "        A_eq = [] # a list which will later be converted to array after appending.\n",
    "        for i in range(len(pi)): # len = number of words.\n",
    "            A = np.zeros_like(D) # a 2d array made with the shape of D.  \n",
    "            A[i, :] = 1 \n",
    "            #print(\"Dshape, len pi till here :\",D.shape,len(pi),\"\\n\")\n",
    "            \n",
    "            # to make summation over \"i\" of Tij = pi, ie total / sum of outflow\n",
    "            ## from one word is equal to its pi (normalized bag of word/ frequency/density)\n",
    "            ## ex : if 2x3 D:\n",
    "            ##T1,1 + T1,2 + T1,3 + 0 T2,1 + 0 T2,2 + 0 T2,3 = P1 and so on for every i,\n",
    "            ## ie for each word in the doc1\n",
    "            \n",
    "            \n",
    "            #print(\"A.shape\", A.shape,\"\\n\")\n",
    "            A_eq.append(A.reshape(-1)) ## reshape(-1) flatens and then appending in A_eq.\n",
    "            \n",
    "            #print(A_eq,\"Aeq\\n\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            ## A_eq will be (m+n)x(m.n)\n",
    "    \n",
    "        for i in range(len(qj)):\n",
    "            A = np.zeros_like(D)\n",
    "            A[:, i] = 1 ## summation over \"j\" this time, so this time for different rows, \n",
    "            ## over a column \"j\" which refers to doc2, ie total incoming flow = qj density\n",
    "            A_eq = list(A_eq)\n",
    "            A_eq.append(A.reshape(-1))\n",
    "            A_eq = np.array(A_eq)\n",
    "        \n",
    "        #print(A_eq.shape,A_eq)\n",
    "       \n",
    "        b_eq = np.concatenate([pi, qj])\n",
    "        D = D.reshape(-1)\n",
    "        #print(\"Dshape:\",D.shape)\n",
    "        if cost == 'max':\n",
    "            D = D*(-1)\n",
    "        \n",
    "        result = linprog(D, A_eq=A_eq[:-1], b_eq=b_eq[:-1]) ## removing redundant to make \n",
    "        ## solution more robust.\n",
    "        return np.absolute(result.fun), result.x , D.reshape((len(pi),len(qj)))  ## fun returns the final optimized value, x returns each value of xi,j that is the array\n",
    "\n",
    "    \n",
    "def relaxed_distance(pi,qj,D,cost='min'):\n",
    "    \n",
    "    # to find relaxed we just add the min/max cost directly using the least distance for pi to qj.\n",
    "    \n",
    "    # D is calculated from P to Q ie P in rows and Q in columns, To find Q to P we will transpose \n",
    "    if cost == 'min':\n",
    "        p_to_q = np.dot(D.min(axis=1),pi)\n",
    "        q_to_p = np.dot(D.T.min(axis=1),qj)\n",
    "        \n",
    "        return max(p_to_q,q_to_p)\n",
    "    \n",
    "    if cost == 'max':\n",
    "        \n",
    "        p_to_q = np.dot(D.max(axis=1),pi)\n",
    "        q_to_p = np.dot(D.T.max(axis=1),qj)\n",
    "        \n",
    "        return min(p_to_q,q_to_p), None, D\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "class WMD:\n",
    "    \n",
    "    ''' wmd type = normal/relaxed, costtype = min/max.\n",
    "    Enter Two sentence strings, cost = max if you want to try \n",
    "    max cost max flow version, embeddingtype = 1 for word2vec, 2 = normalized\n",
    "    word2vec, 3 = glove300d, 4 = glove200d, 5 = glove100d 6 = glove50d'''\n",
    "    \n",
    "    def __init__(self,embeddingtype, wmd_type = 'normal', costtype='min'):\n",
    "        \n",
    "        \n",
    "        self.cost = costtype\n",
    "        \n",
    "        self.embeddingtype = embeddingtype \n",
    "        self.wmd_type = wmd_type\n",
    "        \n",
    "        \n",
    "        ## setting up the embeddings\n",
    "        \n",
    "        embeddings_setup(self.embeddingtype)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    #def word_count(self):\n",
    "#         self.sent1_dic = defaultdict(int)\n",
    "#         self.sent2_dic = defaultdict(int)\n",
    "        \n",
    "#         for word in sorted(sentence_preprocess(self.sent1)):\n",
    "#             self.sent1_dic[word] += 1\n",
    "            \n",
    "#         for word in sorted(sentence_preprocess(self.sent2)):\n",
    "#             self.sent2_dic[word] += 1\n",
    "        \n",
    "#         return dict(self.sent1_dic), dict(self.sent2_dic)\n",
    "\n",
    "\n",
    "\n",
    "#     def wasserstein_distance(self, pi, qj, D):\n",
    "#         \"\"\"Find Wasserstein distance through linear programming\n",
    "#         p.shape=[m], q.shape=[n], D.shape=[m, n]\n",
    "    \n",
    "#         suppose doc1 has m words and doc2 has n words, then an mxn array would be formed, \n",
    "#         having distance of each word in doc1 to that of doc2.\n",
    "    \n",
    "    \n",
    "    \n",
    "#         p.sum()=1, q.sum()=1, p∈[0,1], q∈[0,1]\n",
    "#         \"\"\"\n",
    "#         A_eq = [] # a list which will later be converted to array after appending.\n",
    "#         for i in range(len(pi)): # len = number of words.\n",
    "#             A = np.zeros_like(D) # a 2d array made with the shape of D.  \n",
    "#             A[i, :] = 1 \n",
    "#             # to make summation over \"i\" of Tij = pi, ie total / sum of outflow\n",
    "            ## from one word is equal to its pi (normalized bag of word/ frequency/density)\n",
    "            ## ex : if 2x3 D:\n",
    "            ##T1,1 + T1,2 + T1,3 + 0 T2,1 + 0 T2,2 + 0 T2,3 = P1 and so on for every i,\n",
    "            ## ie for each word in the doc1\n",
    "        \n",
    "#             A_eq.append(A.reshape(-1)) ## reshape(-1) flatens and then appending in A_eq.\n",
    "            ## A_eq will be (m+n)x(m.n)\n",
    "    \n",
    "#         for i in range(len(qj)):\n",
    "#             A = np.zeros_like(D)\n",
    "#             A[:, i] = 1 ## summation over \"j\" this time, so this time for different rows, \n",
    "#             ## over a column \"j\" which refers to doc2, ie total incoming flow = qj density\n",
    "#             A_eq.append(A.reshape(-1))\n",
    "#             A_eq = np.array(A_eq)\n",
    "        \n",
    "#         print(A_eq.shape,A_eq)\n",
    "       \n",
    "#         b_eq = np.concatenate([pi, qj])\n",
    "#         D = D.reshape(-1)\n",
    "#         if self.cost == 'max':\n",
    "#             D = D*(-1)\n",
    "        \n",
    "#         result = linprog(D, A_eq=A_eq[:-1], b_eq=b_eq[:-1]) ## removing redundant to make \n",
    "#         ## solution more robust.\n",
    "#         return result.fun, result.x  ## fun returns the final optimized value, x returns each value of xi,j that is the array\n",
    "\n",
    "    \n",
    "    def word_mover_distance(self,sentence1,sentence2):\n",
    "        \n",
    "        self.sent1 = sentence1\n",
    "        #print(self.sent1 ,\"\\n\")\n",
    "        self.sent2 = sentence2\n",
    "        #print(self.sent2 ,\"\\n\")\n",
    "        \n",
    "        \n",
    "        self.sent1_dic = defaultdict(int)\n",
    "        self.sent2_dic = defaultdict(int)\n",
    "        \n",
    "        for word in sorted(sentence_preprocess(embd_model,self.sent1)): # sorted to have better\n",
    "            self.sent1_dic[word] += 1 # idea of the sequence of the words. Creating BOW here\n",
    "            \n",
    "        for word in sorted(sentence_preprocess(embd_model,self.sent2)): #creating BOW from sorted sequence\n",
    "            self.sent2_dic[word] += 1\n",
    "        \n",
    "        \n",
    "        self.sent1_dic = dict(self.sent1_dic) # converted from default dict to dict.\n",
    "        self.sent2_dic = dict(self.sent2_dic) # because following operations work on dict\n",
    "        \n",
    "        \n",
    "        #print(self.sent1_dic ,\"\\n\")\n",
    "        #print(self.sent2_dic ,\"\\n\")\n",
    "        \n",
    "        \n",
    "        ## Now we will store a list/array of all the words in each sentence (in alphabetically sorted order)\n",
    "        ## we will store corresponding count, and then corresponding Normalised count.\n",
    "        self.sent1_words = np.array(list(self.sent1_dic.keys())) #dictionary keys converted to list than array\n",
    "        self.sent1_counts = np.array(list(self.sent1_dic.values()))\n",
    "        \n",
    "        self.sent2_words = np.array(list(self.sent2_dic.keys()))\n",
    "        self.sent2_counts = np.array(list(self.sent2_dic.values()))\n",
    "        \n",
    "        \n",
    "        #print(self.sent1_words ,\"\\n\")\n",
    "        #print(self.sent1_counts ,\"\\n\")\n",
    "        \n",
    "        #print(self.sent2_words ,\"\\n\")\n",
    "        #print(self.sent2_counts ,\"\\n\")\n",
    "        \n",
    "        #dictionary values cant be converted into an array directly, hence the\n",
    "        #list step.\n",
    "        \n",
    "        #print(\"embedding type being passed is :\", self.embeddingtype,\"\\n\")\n",
    "        self.sent1_embmtx = find_embdMatrix(self.sent1_words)\n",
    "        #print(self.sent1_embmtx.shape,\"sent1emb\\n\")\n",
    "        self.sent2_embmtx = find_embdMatrix(self.sent2_words)\n",
    "        #print(self.sent2_embmtx.shape,\"sent2emb\\n\")\n",
    "        \n",
    "        self.pi = self.sent1_counts/np.sum(self.sent1_counts) #NBOW step from BOW\n",
    "        #print(self.pi,\"self.pi\\n\")\n",
    "        self.qj = self.sent2_counts/np.sum(self.sent2_counts)\n",
    "        #print(self.qj,\"self.qj\\n\")\n",
    "        \n",
    "        self.D = np.sqrt(np.square(self.sent1_embmtx[:, None] - self.sent2_embmtx[None, :]).sum(axis=2)) \n",
    "        #print(self.D.shape,\"Dshape \\n\")\n",
    "        ## programmers sought used mean instead of sum.\n",
    "        ## scipy cdist can be used as well.\n",
    "        \n",
    "        if self.wmd_type == 'normal':\n",
    "            return wasserstein_distance(self.pi, self.qj, self.D, self.cost)\n",
    "        \n",
    "        \n",
    "        if self.wmd_type == 'relaxed':\n",
    "            return relaxed_distance(self.pi,self.qj,self.D,self.cost)\n",
    "        \n",
    "print(\"\\n FUNCTIONS DEFINITION OVER AND DATA LOADING STARTED\\n\")\n",
    "\n",
    "\n",
    "## KNN\n",
    "\n",
    "Train_BBCsport_sent = np.load(\"/scratch/Amit_Pandey/wmd_lite/files/Train_BBCsport_sent.npy\")\n",
    "Train_BBCsport_label = np.load(\"/scratch/Amit_Pandey/wmd_lite/files/Train_BBCsport_label.npy\")\n",
    "Test_BBCsport_sent = np.load(\"/scratch/Amit_Pandey/wmd_lite/files/Test_BBCsport_sent.npy\")\n",
    "Test_BBCsport_label = np.load(\"/scratch/Amit_Pandey/wmd_lite/files/Test_BBCsport_label.npy\")\n",
    "\n",
    "\n",
    "print(\"\\n DATA LOADING ENDED\\n\")\n",
    "\n",
    "#for i in range(5):\n",
    "    #print(Test_BBCsport_label[i],\"\\n\",Test_BBCsport_sent[i])\n",
    "    \n",
    "\n",
    "print(\"##################Train details:\\n\")\n",
    "\n",
    "#for i in range(5):\n",
    "    #print(Train_BBCsport_label[i],\"\\n\",Train_BBCsport_sent[i])\n",
    "\n",
    "\n",
    "# embeddingtype = 3\n",
    "# model = WMD(embeddingtype,wmd_type = 'relaxed', costtype='max')\n",
    "\n",
    "            \n",
    "    \n",
    "no_testdocs = len(Test_BBCsport_sent)\n",
    "no_testlabels = len(Test_BBCsport_label)\n",
    "#no_testdocs,no_testlabels\n",
    "\n",
    "\n",
    "\n",
    "actual_category = []\n",
    "predicted_category = []\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#import time\n",
    "st = time.time()\n",
    "print(\"\\n MODEL INITIALIZATION STARTED\\n\")\n",
    "embeddingtype = 3\n",
    "model = WMD(embeddingtype,wmd_type = 'normal', costtype='min')\n",
    "\n",
    "print(\"\\n MODEL INITIALIZATION OVER\\n\")\n",
    "\n",
    "result_Dhawal = []\n",
    "test_finished_Dhawal = []\n",
    "\n",
    "\n",
    "def predict_Category(i):\n",
    "    global result \n",
    "    global test_finished\n",
    "    \n",
    "    prediction_dictionary = {}\n",
    "    sentence = Test_BBCsport_sent[i]\n",
    "    \n",
    "    distance_fromTrainset = []\n",
    "    \n",
    "    for j in range (len(Train_BBCsport_sent)):\n",
    "    #for j in range (10): #number of train\n",
    "        ## Find totalcost ie distance between sentence passed from test set to each sentence \n",
    "        ## in training set. and then append in the list.\n",
    "        \n",
    "        #print(sentence)\n",
    "        #print(Train_BBCsport_sent[i])\n",
    "        \n",
    "        print(f\"\\nTrain{j}\")\n",
    "        \n",
    "        Totalcost, Tcoeff, Distancematx = model.word_mover_distance(sentence,Train_BBCsport_sent[j])\n",
    "        print(f\" distance btwn test{i} and train{j} :\", Totalcost,\"\\n\")\n",
    "        #print(Totalcost)\n",
    "        distance_fromTrainset.append(Totalcost)\n",
    "        \n",
    "    distance_fromTrainset = np.array(distance_fromTrainset)\n",
    "    #print('distance from train set array:',distance_fromTrainset)\n",
    "    \n",
    "    arr1indx = distance_fromTrainset.argsort()\n",
    "    print(\"index of distance in increasing order is:\", arr1indx, \"\\n\")\n",
    "    \n",
    "    #print(\"Distance and label sorted from test set\",distance_fromTrainset[arr1indx[::1]], \"\\n\",Train_BBCsport_label[arr1indx[::1]],\"\\n\",\"Sentences: \\n\",Train_BBCsport_sent[arr1indx[::1]]) \n",
    "    \n",
    "    \n",
    "    ## Taking for different values of K\n",
    "    \n",
    "    #k = 5\n",
    "    sorted_distance_fromTrainset_k5 = distance_fromTrainset[arr1indx[::1]][:5]\n",
    "    sorted_labels_k5 = Train_BBCsport_label[arr1indx[::1]][:5]\n",
    "    \n",
    "    predicted_cat_k5 = scipy.stats.mode(sorted_labels_k5)[0]\n",
    "    #print(\"pred 5\",predicted_cat_k5)\n",
    "   \n",
    "\n",
    "    #k = 7\n",
    "    sorted_distance_fromTrainset_k7 = distance_fromTrainset[arr1indx[::1]][:7]\n",
    "    sorted_labels_k7 = Train_BBCsport_label[arr1indx[::1]][:7]\n",
    "    \n",
    "    predicted_cat_k7 = scipy.stats.mode(sorted_labels_k7)[0]\n",
    "    #print(\"pred 7\",predicted_cat_k7)\n",
    "\n",
    "    #k = 11\n",
    "    sorted_distance_fromTrainset_k11 = distance_fromTrainset[arr1indx[::1]][:11]\n",
    "    sorted_labels_k11 = Train_BBCsport_label[arr1indx[::1]][:11]\n",
    "    \n",
    "    predicted_cat_k11 = scipy.stats.mode(sorted_labels_k11)[0]\n",
    "    #print(\"pred 11\",predicted_cat_k11)\n",
    "\n",
    "    #k = 15\n",
    "    sorted_distance_fromTrainset_k15 = distance_fromTrainset[arr1indx[::1]][:15]\n",
    "    sorted_labels_k15 = Train_BBCsport_label[arr1indx[::1]][:15]\n",
    "    \n",
    "    predicted_cat_k15 = scipy.stats.mode(sorted_labels_k15)[0]\n",
    "    #print(\"pred 15\",predicted_cat_k15)\n",
    "\n",
    "    #k = 21\n",
    "    sorted_distance_fromTrainset_k21 = distance_fromTrainset[arr1indx[::1]][:21]\n",
    "    sorted_labels_k21 = Train_BBCsport_label[arr1indx[::1]][:21]\n",
    "    \n",
    "    predicted_cat_k21 = scipy.stats.mode(sorted_labels_k21)[0]\n",
    "    #print(\"pred 21\",predicted_cat_k21)\n",
    "\n",
    "\n",
    "    #print(sorted_distance_fromTrainset,sorted_labels)\n",
    "    prediction_dictionary[i] = [Test_BBCsport_label[i],\n",
    "                                arr1indx[:30].tolist(),\n",
    "                                Train_BBCsport_label[arr1indx[::1]][:30].tolist(),\n",
    "                                distance_fromTrainset[arr1indx[::1]][:30].tolist(),\n",
    "                                [predicted_cat_k5.tolist(),predicted_cat_k7.tolist(), predicted_cat_k11.tolist(),predicted_cat_k15.tolist(),\n",
    "                                 predicted_cat_k21.tolist()]]\n",
    "    \n",
    "    \n",
    "    result_Dhawal.append(prediction_dictionary)\n",
    "    test_finished_Dhawal.append(i)\n",
    "    \n",
    "    \n",
    "    with open('../results/result_Dhawal.pickle', 'wb') as handle:\n",
    "        pickle.dump(result_Dhawal, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    with open('../results/test_finished_Dhawal.pickle', 'wb') as handle:\n",
    "        pickle.dump(test_finished_Dhawal, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #return np.array([predicted_cat_k5,predicted_cat_k7,predicted_cat_k11,predicted_cat_k15,predicted_cat_k21])   \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "no_testdocs = len(Test_BBCsport_sent)\n",
    "no_testlabels = len(Test_BBCsport_label)\n",
    "#no_testdocs,no_testlabels\n",
    "\n",
    "\n",
    "\n",
    "actual_categories = []\n",
    "predicted_categories_list = []\n",
    "for i in range (50,220): # number of test\n",
    "    print(f\" \\n ################### Test{i} #############\",\"\\n\")\n",
    "    predict_Category(i)\n",
    "    \n",
    "    #actual_categories.append(Test_BBCsport_label[i]) \n",
    "    #pred_category = predict_Category(Test_BBCsport_sent[i])\n",
    "    #print(pred_category)\n",
    "    #predicted_categories_list.append(pred_category)\n",
    "    \n",
    "\n",
    "et = time.time() \n",
    "\n",
    "print(\"time taken:\",et-st)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "   \n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
