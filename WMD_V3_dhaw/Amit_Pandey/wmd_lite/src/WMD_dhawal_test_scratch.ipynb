{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dbc521d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code started \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Code started \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9123665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.data.path.append(\"/scratch/Amit_Pandey/nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a7fdbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home2/dhawals1939/nltk_data',\n",
       " '/home2/dhawals1939/miniconda3/nltk_data',\n",
       " '/home2/dhawals1939/miniconda3/share/nltk_data',\n",
       " '/home2/dhawals1939/miniconda3/lib/nltk_data',\n",
       " '/usr/share/nltk_data',\n",
       " '/usr/local/share/nltk_data',\n",
       " '/usr/lib/nltk_data',\n",
       " '/usr/local/lib/nltk_data',\n",
       " '/scratch/Amit_Pandey/nltk_data']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.data.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c295cad8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check 1: Imports done\n",
      "\n",
      "check 2 : function definitions \n",
      "\n",
      " loading train and test dataset\n",
      "\n",
      "check 3 : dataset loaded \n",
      "\n",
      " No of test docs and labels loaded: 222   222 \n",
      "\n",
      " No of train docs and labels loaded: 514   514 \n",
      "\n",
      " Model initialization started wtih NORMAL WMD with MIN cost and glove vectors  \n",
      "\n",
      " check 4: model initialization successful \n",
      "\n",
      "running test2 \n",
      "running test5 \n",
      "running test0 \n",
      "running test1 \n",
      "running test4 \n",
      "running test3 \n",
      "running test7 \n",
      "running test6 \n",
      "running test10 \n",
      "running test9 \n",
      "running test8 \n",
      "running test11 \n",
      "running test12 \n",
      "running test13 \n",
      "running test14 \n",
      "running test15 \n",
      "\n",
      "running test16 \n",
      "running test18 \n",
      "running test17 \n",
      "running test19 \n",
      "running test21 \n",
      "running test20 \n",
      "running test22 \n",
      "running test23 \n",
      "\n",
      "\n",
      "\n",
      "running test24 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## To run as batch job\n",
    "\n",
    "\n",
    "#imports:\n",
    "\n",
    "# file imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "import os\n",
    "from scipy.optimize import linprog\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from gensim.models import KeyedVectors\n",
    "import sklearn\n",
    "import scipy\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "print(\"check 1: Imports done\\n\")\n",
    "\n",
    "def sentence_preprocess(embed_dict, sentence,lowercase = 1, strip_punctuation = 1,  remove_stopwords = 1,removedigit = 1):\n",
    "    ''' 1 : True, 0 : False : Lowercase, Strip puncutation, Remove Stopwords, removedigit'''\n",
    "\n",
    "    stop_words = list(stopwords.words('english'))\n",
    "\n",
    "    if lowercase == 1:\n",
    "        sentence = sentence.lower()\n",
    "\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "    if strip_punctuation == 1 and removedigit == 1:\n",
    "        sentence_words = [word for word in sentence_words if word.isalpha()] \n",
    "        \n",
    "\n",
    "\n",
    "    if remove_stopwords == 1:\n",
    "        sentence_words = [word for word in sentence_words if not word in stop_words]\n",
    "    \n",
    "    ## to remove those words which are not in the embeddings that we have.\n",
    "    \n",
    "    sentence_words = [word for word in sentence_words if word in embed_dict.keys()]\n",
    "\n",
    "\n",
    "\n",
    "    return sentence_words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "embeddingtype = None\n",
    "embd_model = None\n",
    "\n",
    "\n",
    "\n",
    "## to load from embedding text files:\n",
    "## have used this to load glove vectors and not word2vec\n",
    "\n",
    "def load_glove(embeddingtype):\n",
    "    \n",
    "    if embeddingtype == 3:\n",
    "        i = 300\n",
    "    if embeddingtype == 4:\n",
    "        i = 200\n",
    "    if embeddingtype == 5:\n",
    "        i = 100\n",
    "    if embeddingtype == 6:\n",
    "        i = 50\n",
    "    \n",
    "    \n",
    "    embeddings_dict = defaultdict(lambda:np.zeros(i)) \n",
    "    # defaultdict to take care of OOV words.\n",
    "    \n",
    "    with open(f\"/scratch/Amit_Pandey/wmd_lite/files/glove.6B.{i}d.txt\",'r') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "        \n",
    "    return embeddings_dict\n",
    "\n",
    "\n",
    "def embeddings_setup(newembeddingtype):\n",
    "    \n",
    "    \n",
    "    global embeddingtype\n",
    "    global embd_model\n",
    "    \n",
    "    \n",
    "    '''to avoid loading all the embeddings in the memory.'''\n",
    "    \n",
    "    ''''## Note : we are finding the embd matrix two times, ie once for each sentence in\n",
    "        ## the pair of sentences.\n",
    "        ## so this happens that embedding type is changed when find_embmatrix is called\n",
    "        ## by the first sentence.\n",
    "        The above line doesnt matter now as we not calling find_embmatrix , instead we setting up.\n",
    "    '''\n",
    "        \n",
    "        \n",
    "        \n",
    "    if ( embeddingtype != newembeddingtype):\n",
    "        #print(\"embdtype  entered :\", embeddingtype != newembtype,\"\\n\")\n",
    "        #print(\"embd_model type changed to :\", type(embd_model),\"\\n\" )\n",
    "        \n",
    "        embeddingtype = newembeddingtype\n",
    "        \n",
    "        #embd_model = embeddings_setup(embeddingtype) #adictionary\n",
    "        \n",
    "        #print(\"embd_model type changed to :\", type(embd_model),\"\\n\" )\n",
    "        #to make sure that we don't download the embeddings again and again,\n",
    "        # we will check if the embedding type is same as the old one\n",
    "        # and update global embd_model, vrna next time vo use hi nhi ho payega.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if embeddingtype == 1:\n",
    "        embedding = KeyedVectors.load('google300w2v.kv', mmap='r')\n",
    "        ## This will be slower but will prevent kernel from crashing.\n",
    "        \n",
    "        ## comment the above line and uncomment this if you have sufficient RAM:\n",
    "        \n",
    "        #w2v_emb = gensim.downloader.load('word2vec-google-news-300')\n",
    "        \n",
    "    if embeddingtype == 2:\n",
    "        print('Normalised word2vec not loaded, will get it soon')\n",
    "        embedding = None\n",
    "    \n",
    "    if embeddingtype in (3,4,5,6):\n",
    "        embedding = load_glove(embeddingtype)\n",
    "        \n",
    "    \n",
    "    embd_model = embedding\n",
    "    \n",
    "    \n",
    "        \n",
    "def find_embdMatrix(sentence):\n",
    "    global embeddingtype\n",
    "    global embd_model\n",
    "    #print(\" global embedding type being passed is :\", embeddingtype,\"\\n\")\n",
    "    #print(\"embedding type received by the find emb matrix is :\", newembtype,\"\\n\")\n",
    "    #print(\"embd model type is :\", type(embd_model),\"\\n\")\n",
    "    \n",
    "    sent_mtx = []\n",
    "    \n",
    "    \n",
    "    ##commented lines moved to embedding setup.\n",
    "    \n",
    "#     ''''## Note : we are finding the embd matrix two times, ie once for each sentence in\n",
    "#     ## the pair of sentences.\n",
    "#     ## so this happens that embedding type is changed when find_embmatrix is called\n",
    "#     ## by the first sentence\n",
    "#     '''\n",
    "#     if ( embeddingtype != newembtype):\n",
    "#         print(\"if embdtype part entered :\", embeddingtype != newembtype,\"\\n\")\n",
    "        \n",
    "#         embeddingtype = newembtype\n",
    "#         embd_model = embeddings_setup(embeddingtype) #adictionary\n",
    "        \n",
    "#         print(\"embd_model type changed to :\", type(embd_model),\"\\n\" )\n",
    "#     #to make sure that we don't download the embeddings again and again,\n",
    "#     # we will check if the embedding type is same as the old one\n",
    "#     # and update global embd_model, vrna next time vo use hi nhi ho payega.\n",
    "    \n",
    "    #print(\"embd_model type changed to :\", type(embd_model),\"\\n\" )\n",
    "    for word in sentence:\n",
    "        word_emb = embd_model[word]\n",
    "        sent_mtx.append(word_emb)\n",
    "    \n",
    "    sent_mtx = np.array(sent_mtx).reshape(len(sentence),-1)\n",
    "\n",
    "    return sent_mtx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def wasserstein_distance(pi, qj, D, cost = 'min'):\n",
    "        \"\"\"Find Wasserstein distance through linear programming\n",
    "        p.shape=[m], q.shape=[n], D.shape=[m, n]\n",
    "    \n",
    "        suppose doc1 has m words and doc2 has n words, then an mxn array would be formed, \n",
    "        having distance of each word in doc1 to that of doc2.\n",
    "    \n",
    "    \n",
    "    \n",
    "        p.sum()=1, q.sum()=1, p∈[0,1], q∈[0,1]\n",
    "        \"\"\"\n",
    "        A_eq = [] # a list which will later be converted to array after appending.\n",
    "        for i in range(len(pi)): # len = number of words.\n",
    "            A = np.zeros_like(D) # a 2d array made with the shape of D.  \n",
    "            A[i, :] = 1 \n",
    "            #print(\"Dshape, len pi till here :\",D.shape,len(pi),\"\\n\")\n",
    "            \n",
    "            # to make summation over \"i\" of Tij = pi, ie total / sum of outflow\n",
    "            ## from one word is equal to its pi (normalized bag of word/ frequency/density)\n",
    "            ## ex : if 2x3 D:\n",
    "            ##T1,1 + T1,2 + T1,3 + 0 T2,1 + 0 T2,2 + 0 T2,3 = P1 and so on for every i,\n",
    "            ## ie for each word in the doc1\n",
    "            \n",
    "            \n",
    "            #print(\"A.shape\", A.shape,\"\\n\")\n",
    "            A_eq.append(A.reshape(-1)) ## reshape(-1) flatens and then appending in A_eq.\n",
    "            \n",
    "            #print(A_eq,\"Aeq\\n\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            ## A_eq will be (m+n)x(m.n)\n",
    "    \n",
    "        for i in range(len(qj)):\n",
    "            A = np.zeros_like(D)\n",
    "            A[:, i] = 1 ## summation over \"j\" this time, so this time for different rows, \n",
    "            ## over a column \"j\" which refers to doc2, ie total incoming flow = qj density\n",
    "            A_eq = list(A_eq)\n",
    "            A_eq.append(A.reshape(-1))\n",
    "            A_eq = np.array(A_eq)\n",
    "        \n",
    "        #print(A_eq.shape,A_eq)\n",
    "       \n",
    "        b_eq = np.concatenate([pi, qj])\n",
    "        D = D.reshape(-1)\n",
    "        #print(\"Dshape:\",D.shape)\n",
    "        if cost == 'max':\n",
    "            D = D*(-1)\n",
    "        \n",
    "        result = linprog(D, A_eq=A_eq[:-1], b_eq=b_eq[:-1]) ## removing redundant to make \n",
    "        ## solution more robust.\n",
    "        return np.absolute(result.fun), result.x , D.reshape((len(pi),len(qj)))  ## fun returns the final optimized value, x returns each value of xi,j that is the array\n",
    "\n",
    "    \n",
    "def relaxed_distance(pi,qj,D,cost='min'):\n",
    "    \n",
    "    # to find relaxed we just add the min/max cost directly using the least distance for pi to qj.\n",
    "    \n",
    "    # D is calculated from P to Q ie P in rows and Q in columns, To find Q to P we will transpose \n",
    "    if cost == 'min':\n",
    "        p_to_q = np.dot(D.min(axis=1),pi)\n",
    "        q_to_p = np.dot(D.T.min(axis=1),qj)\n",
    "        \n",
    "        return max(p_to_q,q_to_p)\n",
    "    \n",
    "    if cost == 'max':\n",
    "        \n",
    "        p_to_q = np.dot(D.max(axis=1),pi)\n",
    "        q_to_p = np.dot(D.T.max(axis=1),qj)\n",
    "        \n",
    "        return min(p_to_q,q_to_p), None, D\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "class WMD:\n",
    "    \n",
    "    ''' wmd type = normal/relaxed, costtype = min/max.\n",
    "    Enter Two sentence strings, cost = max if you want to try \n",
    "    max cost max flow version, embeddingtype = 1 for word2vec, 2 = normalized\n",
    "    word2vec, 3 = glove300d, 4 = glove200d, 5 = glove100d 6 = glove50d'''\n",
    "    \n",
    "    def __init__(self,embeddingtype, wmd_type = 'normal', costtype='min'):\n",
    "        \n",
    "        \n",
    "        self.cost = costtype\n",
    "        \n",
    "        self.embeddingtype = embeddingtype \n",
    "        self.wmd_type = wmd_type\n",
    "        \n",
    "        \n",
    "        ## setting up the embeddings\n",
    "        \n",
    "        embeddings_setup(self.embeddingtype)\n",
    "        \n",
    "        self.res = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    #def word_count(self):\n",
    "#         self.sent1_dic = defaultdict(int)\n",
    "#         self.sent2_dic = defaultdict(int)\n",
    "        \n",
    "#         for word in sorted(sentence_preprocess(self.sent1)):\n",
    "#             self.sent1_dic[word] += 1\n",
    "            \n",
    "#         for word in sorted(sentence_preprocess(self.sent2)):\n",
    "#             self.sent2_dic[word] += 1\n",
    "        \n",
    "#         return dict(self.sent1_dic), dict(self.sent2_dic)\n",
    "\n",
    "\n",
    "\n",
    "#     def wasserstein_distance(self, pi, qj, D):\n",
    "#         \"\"\"Find Wasserstein distance through linear programming\n",
    "#         p.shape=[m], q.shape=[n], D.shape=[m, n]\n",
    "    \n",
    "#         suppose doc1 has m words and doc2 has n words, then an mxn array would be formed, \n",
    "#         having distance of each word in doc1 to that of doc2.\n",
    "    \n",
    "    \n",
    "    \n",
    "#         p.sum()=1, q.sum()=1, p∈[0,1], q∈[0,1]\n",
    "#         \"\"\"\n",
    "#         A_eq = [] # a list which will later be converted to array after appending.\n",
    "#         for i in range(len(pi)): # len = number of words.\n",
    "#             A = np.zeros_like(D) # a 2d array made with the shape of D.  \n",
    "#             A[i, :] = 1 \n",
    "#             # to make summation over \"i\" of Tij = pi, ie total / sum of outflow\n",
    "            ## from one word is equal to its pi (normalized bag of word/ frequency/density)\n",
    "            ## ex : if 2x3 D:\n",
    "            ##T1,1 + T1,2 + T1,3 + 0 T2,1 + 0 T2,2 + 0 T2,3 = P1 and so on for every i,\n",
    "            ## ie for each word in the doc1\n",
    "        \n",
    "#             A_eq.append(A.reshape(-1)) ## reshape(-1) flatens and then appending in A_eq.\n",
    "            ## A_eq will be (m+n)x(m.n)\n",
    "    \n",
    "#         for i in range(len(qj)):\n",
    "#             A = np.zeros_like(D)\n",
    "#             A[:, i] = 1 ## summation over \"j\" this time, so this time for different rows, \n",
    "#             ## over a column \"j\" which refers to doc2, ie total incoming flow = qj density\n",
    "#             A_eq.append(A.reshape(-1))\n",
    "#             A_eq = np.array(A_eq)\n",
    "        \n",
    "#         print(A_eq.shape,A_eq)\n",
    "       \n",
    "#         b_eq = np.concatenate([pi, qj])\n",
    "#         D = D.reshape(-1)\n",
    "#         if self.cost == 'max':\n",
    "#             D = D*(-1)\n",
    "        \n",
    "#         result = linprog(D, A_eq=A_eq[:-1], b_eq=b_eq[:-1]) ## removing redundant to make \n",
    "#         ## solution more robust.\n",
    "#         return result.fun, result.x  ## fun returns the final optimized value, x returns each value of xi,j that is the array\n",
    "\n",
    "    \n",
    "    def word_mover_distance(self,sentence1,sentence2):\n",
    "        \n",
    "        self.sent1 = sentence1\n",
    "        #print(self.sent1 ,\"\\n\")\n",
    "        self.sent2 = sentence2\n",
    "        #print(self.sent2 ,\"\\n\")\n",
    "        \n",
    "        \n",
    "        self.sent1_dic = defaultdict(int)\n",
    "        self.sent2_dic = defaultdict(int)\n",
    "        \n",
    "        for word in sorted(sentence_preprocess(embd_model,self.sent1)): # sorted to have better\n",
    "            self.sent1_dic[word] += 1 # idea of the sequence of the words. Creating BOW here\n",
    "            \n",
    "        for word in sorted(sentence_preprocess(embd_model,self.sent2)): #creating BOW from sorted sequence\n",
    "            self.sent2_dic[word] += 1\n",
    "        \n",
    "        \n",
    "        self.sent1_dic = dict(self.sent1_dic) # converted from default dict to dict.\n",
    "        self.sent2_dic = dict(self.sent2_dic) # because following operations work on dict\n",
    "        \n",
    "        \n",
    "        #print(self.sent1_dic ,\"\\n\")\n",
    "        #print(self.sent2_dic ,\"\\n\")\n",
    "        \n",
    "        \n",
    "        ## Now we will store a list/array of all the words in each sentence (in alphabetically sorted order)\n",
    "        ## we will store corresponding count, and then corresponding Normalised count.\n",
    "        self.sent1_words = np.array(list(self.sent1_dic.keys())) #dictionary keys converted to list than array\n",
    "        self.sent1_counts = np.array(list(self.sent1_dic.values()))\n",
    "        \n",
    "        self.sent2_words = np.array(list(self.sent2_dic.keys()))\n",
    "        self.sent2_counts = np.array(list(self.sent2_dic.values()))\n",
    "        \n",
    "        \n",
    "        #print(self.sent1_words ,\"\\n\")\n",
    "        #print(self.sent1_counts ,\"\\n\")\n",
    "        \n",
    "        #print(self.sent2_words ,\"\\n\")\n",
    "        #print(self.sent2_counts ,\"\\n\")\n",
    "        \n",
    "        #dictionary values cant be converted into an array directly, hence the\n",
    "        #list step.\n",
    "        \n",
    "        #print(\"embedding type being passed is :\", self.embeddingtype,\"\\n\")\n",
    "        self.sent1_embmtx = find_embdMatrix(self.sent1_words)\n",
    "        #print(self.sent1_embmtx.shape,\"sent1emb\\n\")\n",
    "        self.sent2_embmtx = find_embdMatrix(self.sent2_words)\n",
    "        #print(self.sent2_embmtx.shape,\"sent2emb\\n\")\n",
    "        \n",
    "        self.pi = self.sent1_counts/np.sum(self.sent1_counts) #NBOW step from BOW\n",
    "        #print(self.pi,\"self.pi\\n\")\n",
    "        self.qj = self.sent2_counts/np.sum(self.sent2_counts)\n",
    "        #print(self.qj,\"self.qj\\n\")\n",
    "        \n",
    "        self.D = np.sqrt(np.square(self.sent1_embmtx[:, None] - self.sent2_embmtx[None, :]).sum(axis=2)) \n",
    "        #print(self.D.shape,\"Dshape \\n\")\n",
    "        ## programmers sought used mean instead of sum.\n",
    "        ## scipy cdist can be used as well.\n",
    "        \n",
    "        if self.wmd_type == 'normal':\n",
    "            return wasserstein_distance(self.pi, self.qj, self.D, self.cost)\n",
    "        \n",
    "        \n",
    "        if self.wmd_type == 'relaxed':\n",
    "            return relaxed_distance(self.pi,self.qj,self.D,self.cost)\n",
    "\n",
    "        \n",
    "\n",
    "print(\"check 2 : function definitions \\n\")\n",
    "\n",
    "## KNN\n",
    "\n",
    "print(\" loading train and test dataset\\n\")\n",
    "\n",
    "Train_BBCsport_sent = np.load(\"/scratch/Amit_Pandey/wmd_lite/files/Train_BBCsport_sent.npy\")\n",
    "Train_BBCsport_label = np.load(\"/scratch/Amit_Pandey/wmd_lite/files/Train_BBCsport_label.npy\")\n",
    "Test_BBCsport_sent = np.load(\"/scratch/Amit_Pandey/wmd_lite/files/Test_BBCsport_sent.npy\")\n",
    "Test_BBCsport_label = np.load(\"/scratch/Amit_Pandey/wmd_lite/files/Test_BBCsport_label.npy\")\n",
    "\n",
    "\n",
    "# for i in range(10):\n",
    "#     print(Test_BBCsport_label[i],\"\\n\",Test_BBCsport_sent[i])\n",
    "    \n",
    "\n",
    "# print(\"##################Train details:\\n\")\n",
    "\n",
    "# for i in range(30):\n",
    "#     print(Train_BBCsport_label[i],\"\\n\",Train_BBCsport_sent[i])\n",
    "\n",
    "\n",
    "# embeddingtype = 3\n",
    "# model = WMD(embeddingtype,wmd_type = 'relaxed', costtype='max')\n",
    "\n",
    "            \n",
    "    \n",
    "no_testdocs = len(Test_BBCsport_sent)\n",
    "no_testlabels = len(Test_BBCsport_label)\n",
    "#no_testdocs,no_testlabels\n",
    "\n",
    "print(\"check 3 : dataset loaded \\n\")\n",
    "\n",
    "print(\" No of test docs and labels loaded:\",no_testdocs,\" \",no_testlabels,\"\\n\")\n",
    "\n",
    "print(\" No of train docs and labels loaded:\",len(Train_BBCsport_sent),\" \",len(Train_BBCsport_label),\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "actual_category = []\n",
    "predicted_category = []\n",
    "prediction_dictionary = {}\n",
    "\n",
    "## prediction_dictionary contains test sentence as key, and [['original lable'],[predicted labels for diff k],\n",
    "## , [index of top labels of train set],['distance of top 30 elements']]\n",
    "\n",
    "    \n",
    "\n",
    "print(\" Model initialization started wtih NORMAL WMD with MIN cost and glove vectors  \\n\")\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "embeddingtype = 3\n",
    "model = WMD(embeddingtype,wmd_type = 'normal', costtype='min')\n",
    "\n",
    "print(\" check 4: model initialization successful \\n\")\n",
    "\n",
    "\n",
    "\n",
    "result = []\n",
    "\n",
    "def predict_Category(i):\n",
    "    \n",
    "    global result \n",
    "    sentence = Test_BBCsport_sent[i]\n",
    "    print(f\"running test{i} \\n\")\n",
    "    #print(\"actual category :\", Test_BBCsport_label[i])\n",
    "    #actual_category.append(Test_BBCsport_label[i])\n",
    "    \n",
    "    distance_fromTrainset = []\n",
    "    \n",
    "    for j in range (len(Train_BBCsport_sent)):\n",
    "        \n",
    "       \n",
    "    #for j in range (10):\n",
    "        #print(f\"Train sent{i} \\n\")\n",
    "        ## Find totalcost ie distance between sentence passed from test set to each sentence \n",
    "        ## in training set. and then append in the list.\n",
    "        \n",
    "        #print(sentence)\n",
    "        #print(Train_BBCsport_sent[i])\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        Totalcost, Tcoeff, Distancematx = model.word_mover_distance(sentence,Train_BBCsport_sent[j])\n",
    "        #print(Totalcost)\n",
    "        distance_fromTrainset.append(Totalcost)\n",
    "        \n",
    "    distance_fromTrainset = np.array(distance_fromTrainset)\n",
    "    #print('distance from train set array:',distance_fromTrainset)\n",
    "    \n",
    "    arr1indx = distance_fromTrainset.argsort()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(\"Original Sentence : \\n\",sentence, \"\\n\",\"Distance and label sorted from test set\\n\",distance_fromTrainset[arr1indx[::1]], \"\\n\",Train_BBCsport_label[arr1indx[::1]],\"\\n\",\" Train Sentences: \\n\",Train_BBCsport_sent[arr1indx[::1]]) \n",
    "    \n",
    "    print(\"Starting distance calculation############################### \\n\")\n",
    "    \n",
    "    ## Taking for different values of K\n",
    "    \n",
    "    #k = 5\n",
    "    sorted_distance_fromTrainset_k5 = distance_fromTrainset[arr1indx[::1]][:5]\n",
    "    sorted_labels_k5 = Train_BBCsport_label[arr1indx[::1]][:5]\n",
    "    \n",
    "    predicted_cat_k5 = scipy.stats.mode(sorted_labels_k5)[0]\n",
    "    print(f\"pred 5 for test {i} \",predicted_cat_k5)\n",
    "   \n",
    "\n",
    "    #k = 7\n",
    "    sorted_distance_fromTrainset_k7 = distance_fromTrainset[arr1indx[::1]][:7]\n",
    "    sorted_labels_k7 = Train_BBCsport_label[arr1indx[::1]][:7]\n",
    "    \n",
    "    predicted_cat_k7 = scipy.stats.mode(sorted_labels_k7)[0]\n",
    "    print(f\"pred 7 for test {i} \",predicted_cat_k7)\n",
    "\n",
    "    #k = 11\n",
    "    sorted_distance_fromTrainset_k11 = distance_fromTrainset[arr1indx[::1]][:11]\n",
    "    sorted_labels_k11 = Train_BBCsport_label[arr1indx[::1]][:11]\n",
    "    \n",
    "    predicted_cat_k11 = scipy.stats.mode(sorted_labels_k11)[0]\n",
    "    print(f\"pred 11 for test {i} \",predicted_cat_k11)\n",
    "\n",
    "    #k = 15\n",
    "    sorted_distance_fromTrainset_k15 = distance_fromTrainset[arr1indx[::1]][:15]\n",
    "    sorted_labels_k15 = Train_BBCsport_label[arr1indx[::1]][:15]\n",
    "    \n",
    "    predicted_cat_k15 = scipy.stats.mode(sorted_labels_k15)[0]\n",
    "    print(f\"pred 15 for test {i} \",predicted_cat_k15)\n",
    "\n",
    "    #k = 21\n",
    "    sorted_distance_fromTrainset_k21 = distance_fromTrainset[arr1indx[::1]][:21]\n",
    "    sorted_labels_k21 = Train_BBCsport_label[arr1indx[::1]][:21]\n",
    "    \n",
    "    predicted_cat_k21 = scipy.stats.mode(sorted_labels_k21)[0]\n",
    "    print(f\"pred 21 for test {i} \",predicted_cat_k21)\n",
    "\n",
    "\n",
    "    #print(sorted_distance_fromTrainset,sorted_labels)\n",
    "\n",
    "    prediction_dictionary[i] = [Test_BBCsport_label[i],\n",
    "                                arr1indx[:30].tolist(),\n",
    "                                Train_BBCsport_label[arr1indx[::1]][:30].tolist(),\n",
    "                                distance_fromTrainset[arr1indx[::1]][:30].tolist(),\n",
    "                                [predicted_cat_k5.tolist(),predicted_cat_k7.tolist(), predicted_cat_k11.tolist(),predicted_cat_k15.tolist(),\n",
    "                                 predicted_cat_k21.tolist()]]\n",
    "    \n",
    "    result.append(prediction_dictionary)\n",
    "    with open('../results/test_result.pickle', 'wb') as handle:\n",
    "        pickle.dump(result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    \n",
    "    return prediction_dictionary\n",
    "\n",
    "    #return np.array([predicted_cat_k5,predicted_cat_k7, predicted_cat_k11,predicted_cat_k15,predicted_cat_k21])#, distance_fromTrainset[arr1indx[::1]],Train_BBCsport_label[arr1indx[::1]]\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "# no_testdocs = len(Test_BBCsport_sent)\n",
    "# no_testlabels = len(Test_BBCsport_label)\n",
    "#no_testdocs,no_testlabels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#predicted_categories_list = []\n",
    "# for i in range (1,2):\n",
    "#     print(Test_BBCsport_label[i])\n",
    "#     actual_categories.append(Test_BBCsport_label[i]) \n",
    "#     pred_category = predict_Category(Test_BBCsport_sent[i])\n",
    "#     print(pred_category)\n",
    "#     predicted_categories_list.append(pred_category)\n",
    "\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "with Pool(25) as p :\n",
    "        predicted_Categorieslist = p.map(predict_Category,range(25))\n",
    "        \n",
    "print(predicted_Categorieslist)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "et = time.time()\n",
    "\n",
    "print(\"\\n time taken: \",et-st)\n",
    "print(\"..................\\n\\n\\n\")\n",
    "print(predicted_Categorieslist)\n",
    "\n",
    "\n",
    "\n",
    "# a_file = open(\"wmdresult.json\", \"w\")\n",
    "# json.dump(predicted_Categorieslist, a_file)\n",
    "# a_file.close()\n",
    "\n",
    "# # a_file = open(\"wmdresult.json\", \"r\")\n",
    "# # output = a_file.read()\n",
    "# # print(output)\n",
    "\n",
    "# # a_file.close()\n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "   \n",
    " \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff7dd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (et-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04e8aeef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"../results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0c55d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[{0: ['cricket', [8, 7, 6, 1, 3, 5, 2, 9, 0, 4], ['cricket', 'football', 'athletics', 'cricket', 'athletics', 'football', 'football', 'football', 'football', 'athletics'], [6.153822888694359, 6.416609366447687, 6.433713475780136, 6.4483796278225745, 6.663270389922386, 6.704780227080336, 6.83206604978979, 6.860877694995498, 6.947852794854834, 7.062810289714471], [['athletics'], ['football'], ['football'], ['football'], ['football']]]}]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "a = predicted_Categorieslist\n",
    "\n",
    "with open('../results/test_result.pickle', 'wb') as handle:\n",
    "    pickle.dump(predicted_Categorieslist, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('../results/test_result.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "\n",
    "print (a == b)\n",
    "print(predicted_Categorieslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e7c48fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.mkdir(\"../../../../../scratch/Amit_Pandey/Random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2864771b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__pycache__',\n",
       " 'GoogleNews-vectors-negative300.bin',\n",
       " '__init__.py',\n",
       " 'word2vec-google-news-300.gz']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"../../../../../scratch/Amit_Pandey/gensim-data/word2vec-google-news-300/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "786e0847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nltk_data',\n",
       " 'Dhawal_Ada_copy.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " 'wmd_lite',\n",
       " 'Random',\n",
       " 'gensim-data']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"/scratch/Amit_Pandey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0fa8d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3486309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "w = models.KeyedVectors.load_word2vec_format(\n",
    "    '/scratch/Amit_Pandey/gensim-data/word2vec-google-news-300/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a28d1954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mccain', 0.731901228427887),\n",
       " ('hillary', 0.7284599542617798),\n",
       " ('obamas', 0.7229632139205933),\n",
       " ('george_bush', 0.7205674648284912),\n",
       " ('barack_obama', 0.7045838832855225),\n",
       " ('palin', 0.7043113708496094),\n",
       " ('clinton', 0.6934447884559631),\n",
       " ('clintons', 0.6816834211349487),\n",
       " ('sarah_palin', 0.6815145015716553),\n",
       " ('john_mccain', 0.6800708174705505)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.most_similar('obama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2cc8b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append(\"/scratch/Amit_Pandey/nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fcd8210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home2/dhawals1939/nltk_data',\n",
       " '/home2/dhawals1939/miniconda3/nltk_data',\n",
       " '/home2/dhawals1939/miniconda3/share/nltk_data',\n",
       " '/home2/dhawals1939/miniconda3/lib/nltk_data',\n",
       " '/usr/share/nltk_data',\n",
       " '/usr/local/share/nltk_data',\n",
       " '/usr/lib/nltk_data',\n",
       " '/usr/local/lib/nltk_data',\n",
       " '/scratch/Amit_Pandey/nltk_data']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.data.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a680fee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/scratch/Amit_Pandey']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "print (glob.glob(\"/scratch/A*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68c1a6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_result.pickle']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"../results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4267ca70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0: ['cricket',\n",
       "   [8, 7, 6, 1, 3, 5, 2, 9, 0, 4],\n",
       "   ['cricket',\n",
       "    'football',\n",
       "    'athletics',\n",
       "    'cricket',\n",
       "    'athletics',\n",
       "    'football',\n",
       "    'football',\n",
       "    'football',\n",
       "    'football',\n",
       "    'athletics'],\n",
       "   [6.153822888694359,\n",
       "    6.416609366447687,\n",
       "    6.433713475780136,\n",
       "    6.4483796278225745,\n",
       "    6.663270389922386,\n",
       "    6.704780227080336,\n",
       "    6.83206604978979,\n",
       "    6.860877694995498,\n",
       "    6.947852794854834,\n",
       "    7.062810289714471],\n",
       "   [['athletics'], ['football'], ['football'], ['football'], ['football']]]},\n",
       " {1: ['tennis',\n",
       "   [6, 9, 5, 7, 3, 1, 8, 4, 2, 0],\n",
       "   ['athletics',\n",
       "    'football',\n",
       "    'football',\n",
       "    'football',\n",
       "    'athletics',\n",
       "    'cricket',\n",
       "    'cricket',\n",
       "    'athletics',\n",
       "    'football',\n",
       "    'football'],\n",
       "   [6.036607361817292,\n",
       "    6.0437403283594495,\n",
       "    6.177583736722037,\n",
       "    6.179901149750329,\n",
       "    6.356040006007032,\n",
       "    6.397477020845434,\n",
       "    6.478305482059028,\n",
       "    6.49276396371214,\n",
       "    6.607054366961158,\n",
       "    6.916004403113476],\n",
       "   [['football'], ['football'], ['football'], ['football'], ['football']]]},\n",
       " {2: ['football',\n",
       "   [9, 7, 5, 6, 0, 2, 1, 3, 8, 4],\n",
       "   ['football',\n",
       "    'football',\n",
       "    'football',\n",
       "    'athletics',\n",
       "    'football',\n",
       "    'football',\n",
       "    'cricket',\n",
       "    'athletics',\n",
       "    'cricket',\n",
       "    'athletics'],\n",
       "   [5.801237808018902,\n",
       "    6.056340429449832,\n",
       "    6.401008168392668,\n",
       "    6.427767250044869,\n",
       "    6.665473288590343,\n",
       "    6.666211413750875,\n",
       "    6.741285672838174,\n",
       "    6.7592211501357955,\n",
       "    6.821519063126428,\n",
       "    6.826556391664442],\n",
       "   [['football'], ['football'], ['football'], ['football'], ['football']]]},\n",
       " {3: ['rugby',\n",
       "   [7, 5, 9, 6, 3, 8, 2, 1, 0, 4],\n",
       "   ['football',\n",
       "    'football',\n",
       "    'football',\n",
       "    'athletics',\n",
       "    'athletics',\n",
       "    'cricket',\n",
       "    'football',\n",
       "    'cricket',\n",
       "    'football',\n",
       "    'athletics'],\n",
       "   [5.925429677677279,\n",
       "    6.013749773677943,\n",
       "    6.078648227549582,\n",
       "    6.122313986236665,\n",
       "    6.234518538636698,\n",
       "    6.245367134856279,\n",
       "    6.468995457982604,\n",
       "    6.616508102771298,\n",
       "    6.6743702763363535,\n",
       "    6.768442584095609],\n",
       "   [['football'], ['football'], ['football'], ['football'], ['football']]]},\n",
       " {4: ['tennis',\n",
       "   [9, 5, 7, 6, 3, 4, 1, 8, 2, 0],\n",
       "   ['football',\n",
       "    'football',\n",
       "    'football',\n",
       "    'athletics',\n",
       "    'athletics',\n",
       "    'athletics',\n",
       "    'cricket',\n",
       "    'cricket',\n",
       "    'football',\n",
       "    'football'],\n",
       "   [6.178188816032227,\n",
       "    6.229778459607193,\n",
       "    6.338638097094764,\n",
       "    6.4562904614634595,\n",
       "    6.559415515075148,\n",
       "    6.65674648130452,\n",
       "    6.695635677350568,\n",
       "    6.768926801646102,\n",
       "    6.89665714012756,\n",
       "    6.9943506601192285],\n",
       "   [['football'], ['athletics'], ['football'], ['football'], ['football']]]},\n",
       " {5: ['athletics',\n",
       "   [6, 9, 4, 3, 5, 7, 8, 1, 2, 0],\n",
       "   ['athletics',\n",
       "    'football',\n",
       "    'athletics',\n",
       "    'athletics',\n",
       "    'football',\n",
       "    'football',\n",
       "    'cricket',\n",
       "    'cricket',\n",
       "    'football',\n",
       "    'football'],\n",
       "   [6.2810538752674,\n",
       "    6.5365769361389425,\n",
       "    6.55290712840188,\n",
       "    6.559712434515459,\n",
       "    6.5716505959233285,\n",
       "    6.648686687779811,\n",
       "    6.7236476188502134,\n",
       "    6.8620256743532435,\n",
       "    6.89642461792779,\n",
       "    6.996100808801902],\n",
       "   [['athletics'], ['athletics'], ['football'], ['football'], ['football']]]},\n",
       " {6: ['football',\n",
       "   [9, 7, 5, 6, 2, 3, 8, 0, 1, 4],\n",
       "   ['football',\n",
       "    'football',\n",
       "    'football',\n",
       "    'athletics',\n",
       "    'football',\n",
       "    'athletics',\n",
       "    'cricket',\n",
       "    'football',\n",
       "    'cricket',\n",
       "    'athletics'],\n",
       "   [6.442610400716134,\n",
       "    6.59082417191185,\n",
       "    6.747977252489605,\n",
       "    6.87834330866966,\n",
       "    6.922668675327721,\n",
       "    6.980166981499865,\n",
       "    7.144824255586137,\n",
       "    7.197023953131772,\n",
       "    7.235113485654675,\n",
       "    7.365472806229397],\n",
       "   [['football'], ['football'], ['football'], ['football'], ['football']]]},\n",
       " {7: ['football',\n",
       "   [9, 7, 5, 6, 2, 8, 1, 3, 0, 4],\n",
       "   ['football',\n",
       "    'football',\n",
       "    'football',\n",
       "    'athletics',\n",
       "    'football',\n",
       "    'cricket',\n",
       "    'cricket',\n",
       "    'athletics',\n",
       "    'football',\n",
       "    'athletics'],\n",
       "   [5.327514765583428,\n",
       "    5.623535302513658,\n",
       "    5.994561876635414,\n",
       "    6.030937778011773,\n",
       "    6.081091904004365,\n",
       "    6.244571901792977,\n",
       "    6.3127751996194,\n",
       "    6.389353323049086,\n",
       "    6.463785248912359,\n",
       "    6.7079523609450495],\n",
       "   [['football'], ['football'], ['football'], ['football'], ['football']]]},\n",
       " {8: ['football',\n",
       "   [5, 7, 9, 8, 6, 3, 2, 4, 0, 1],\n",
       "   ['football',\n",
       "    'football',\n",
       "    'football',\n",
       "    'cricket',\n",
       "    'athletics',\n",
       "    'athletics',\n",
       "    'football',\n",
       "    'athletics',\n",
       "    'football',\n",
       "    'cricket'],\n",
       "   [6.840059102401768,\n",
       "    7.061536805870151,\n",
       "    7.085971326456534,\n",
       "    7.092988704499533,\n",
       "    7.205658639579692,\n",
       "    7.449627837834137,\n",
       "    7.465358111502782,\n",
       "    7.469529850211532,\n",
       "    7.4865332610350315,\n",
       "    7.5159326767903565],\n",
       "   [['football'], ['football'], ['football'], ['football'], ['football']]]},\n",
       " {9: ['football',\n",
       "   [9, 5, 7, 0, 3, 2, 6, 8, 4, 1],\n",
       "   ['football',\n",
       "    'football',\n",
       "    'football',\n",
       "    'football',\n",
       "    'athletics',\n",
       "    'football',\n",
       "    'athletics',\n",
       "    'cricket',\n",
       "    'athletics',\n",
       "    'cricket'],\n",
       "   [5.710082926908463,\n",
       "    5.9636631674120535,\n",
       "    5.970239595689915,\n",
       "    6.151630398033891,\n",
       "    6.2947416187971665,\n",
       "    6.310046900931123,\n",
       "    6.475708009399947,\n",
       "    6.777140153278065,\n",
       "    6.879362753197274,\n",
       "    6.945695481541881],\n",
       "   [['football'], ['football'], ['football'], ['football'], ['football']]]},\n",
       " {10: ['football',\n",
       "   [9, 7, 5, 6, 0, 8, 2, 1, 3, 4],\n",
       "   ['football',\n",
       "    'football',\n",
       "    'football',\n",
       "    'athletics',\n",
       "    'football',\n",
       "    'cricket',\n",
       "    'football',\n",
       "    'cricket',\n",
       "    'athletics',\n",
       "    'athletics'],\n",
       "   [6.509248894488268,\n",
       "    6.5592171356039835,\n",
       "    6.664696769644765,\n",
       "    6.7825718319908415,\n",
       "    6.785139739392936,\n",
       "    6.896211278046929,\n",
       "    7.041527938498889,\n",
       "    7.106721343304875,\n",
       "    7.1219307580640265,\n",
       "    7.2279141220144645],\n",
       "   [['football'], ['football'], ['football'], ['football'], ['football']]]},\n",
       " {11: ['cricket',\n",
       "   [7, 1, 8, 9, 6, 5, 3, 2, 4, 0],\n",
       "   ['football',\n",
       "    'cricket',\n",
       "    'cricket',\n",
       "    'football',\n",
       "    'athletics',\n",
       "    'football',\n",
       "    'athletics',\n",
       "    'football',\n",
       "    'athletics',\n",
       "    'football'],\n",
       "   [6.323866173062981,\n",
       "    6.34582882677216,\n",
       "    6.4027668057454274,\n",
       "    6.437187546289877,\n",
       "    6.676654179989443,\n",
       "    6.7728395435541415,\n",
       "    6.925693931336578,\n",
       "    7.001385104602083,\n",
       "    7.162764658544148,\n",
       "    7.400571323021152],\n",
       "   [['cricket'], ['football'], ['football'], ['football'], ['football']]]},\n",
       " {12: ['tennis',\n",
       "   [9, 6, 5, 0, 3, 7, 8, 4, 1, 2],\n",
       "   ['football',\n",
       "    'athletics',\n",
       "    'football',\n",
       "    'football',\n",
       "    'athletics',\n",
       "    'football',\n",
       "    'cricket',\n",
       "    'athletics',\n",
       "    'cricket',\n",
       "    'football'],\n",
       "   [7.401207129294937,\n",
       "    7.5378893283783945,\n",
       "    7.550757514159681,\n",
       "    7.6006231527005195,\n",
       "    7.6117848769710275,\n",
       "    7.644317606479788,\n",
       "    7.6926877263508775,\n",
       "    7.697113371933636,\n",
       "    7.716782835397315,\n",
       "    7.780575745962196],\n",
       "   [['football'], ['football'], ['football'], ['football'], ['football']]]},\n",
       " {13: ['football',\n",
       "   [7, 9, 0, 2, 3, 5, 6, 1, 4, 8],\n",
       "   ['football',\n",
       "    'football',\n",
       "    'football',\n",
       "    'football',\n",
       "    'athletics',\n",
       "    'football',\n",
       "    'athletics',\n",
       "    'cricket',\n",
       "    'athletics',\n",
       "    'cricket'],\n",
       "   [6.5498195083863475,\n",
       "    6.679221410921272,\n",
       "    6.759662830495171,\n",
       "    6.88941926188718,\n",
       "    6.93555937561521,\n",
       "    6.961719288468238,\n",
       "    6.996520613945735,\n",
       "    7.08364760769283,\n",
       "    7.123040512827055,\n",
       "    7.216834162411601],\n",
       "   [['football'], ['football'], ['football'], ['football'], ['football']]]},\n",
       " {14: ['football',\n",
       "   [9, 7, 5, 6, 0, 2, 8, 3, 1, 4],\n",
       "   ['football',\n",
       "    'football',\n",
       "    'football',\n",
       "    'athletics',\n",
       "    'football',\n",
       "    'football',\n",
       "    'cricket',\n",
       "    'athletics',\n",
       "    'cricket',\n",
       "    'athletics'],\n",
       "   [5.4496609933297275,\n",
       "    5.458639158474164,\n",
       "    5.60679008440294,\n",
       "    5.802100204831562,\n",
       "    5.878138614856298,\n",
       "    6.025701846865649,\n",
       "    6.1365004679356545,\n",
       "    6.249505398942176,\n",
       "    6.4033391492042995,\n",
       "    6.669249870976194],\n",
       "   [['football'], ['football'], ['football'], ['football'], ['football']]]},\n",
       " {15: ['football',\n",
       "   [9, 5, 7, 3, 0, 6, 2, 8, 1, 4],\n",
       "   ['football',\n",
       "    'football',\n",
       "    'football',\n",
       "    'athletics',\n",
       "    'football',\n",
       "    'athletics',\n",
       "    'football',\n",
       "    'cricket',\n",
       "    'cricket',\n",
       "    'athletics'],\n",
       "   [5.641749955638206,\n",
       "    5.9058720502424595,\n",
       "    6.25832872414375,\n",
       "    6.3962543721816445,\n",
       "    6.5094109694183855,\n",
       "    6.532421101534735,\n",
       "    6.594691405981681,\n",
       "    6.607088338474741,\n",
       "    6.828124068793274,\n",
       "    6.924759578611484],\n",
       "   [['football'], ['football'], ['football'], ['football'], ['football']]]},\n",
       " {16: ['football',\n",
       "   [5, 7, 6, 9, 2, 8, 3, 1, 4, 0],\n",
       "   ['football',\n",
       "    'football',\n",
       "    'athletics',\n",
       "    'football',\n",
       "    'football',\n",
       "    'cricket',\n",
       "    'athletics',\n",
       "    'cricket',\n",
       "    'athletics',\n",
       "    'football'],\n",
       "   [5.6551205951922245,\n",
       "    5.698429819153452,\n",
       "    5.831396447032628,\n",
       "    5.9437719262372735,\n",
       "    6.087857428537418,\n",
       "    6.22658377471087,\n",
       "    6.287886633257836,\n",
       "    6.481919234914751,\n",
       "    6.707629228668902,\n",
       "    6.77586394729441],\n",
       "   [['football'], ['football'], ['football'], ['football'], ['football']]]},\n",
       " {17: ['athletics',\n",
       "   [2, 3, 4, 6, 7, 1, 5, 9, 8, 0],\n",
       "   ['football',\n",
       "    'athletics',\n",
       "    'athletics',\n",
       "    'athletics',\n",
       "    'football',\n",
       "    'cricket',\n",
       "    'football',\n",
       "    'football',\n",
       "    'cricket',\n",
       "    'football'],\n",
       "   [6.265083220937257,\n",
       "    6.365927848768365,\n",
       "    6.384147928650549,\n",
       "    6.44994525103239,\n",
       "    6.712172376146946,\n",
       "    6.726187432580267,\n",
       "    7.0674058811935865,\n",
       "    7.140824594462914,\n",
       "    7.237632489466864,\n",
       "    7.457681573573159],\n",
       "   [['athletics'], ['athletics'], ['football'], ['football'], ['football']]]},\n",
       " {18: ['cricket',\n",
       "   [1, 7, 6, 2, 4, 5, 3, 8, 9, 0],\n",
       "   ['cricket',\n",
       "    'football',\n",
       "    'athletics',\n",
       "    'football',\n",
       "    'athletics',\n",
       "    'football',\n",
       "    'athletics',\n",
       "    'cricket',\n",
       "    'football',\n",
       "    'football'],\n",
       "   [6.490251720060819,\n",
       "    6.570103585860568,\n",
       "    6.708110571524772,\n",
       "    6.724568562996989,\n",
       "    6.84442121510929,\n",
       "    6.88851996753324,\n",
       "    6.914372769496227,\n",
       "    7.083689825447126,\n",
       "    7.128826694512011,\n",
       "    7.22009671133413],\n",
       "   [['athletics'], ['athletics'], ['football'], ['football'], ['football']]]},\n",
       " {19: ['rugby',\n",
       "   [8, 6, 9, 5, 7, 3, 2, 1, 4, 0],\n",
       "   ['cricket',\n",
       "    'athletics',\n",
       "    'football',\n",
       "    'football',\n",
       "    'football',\n",
       "    'athletics',\n",
       "    'football',\n",
       "    'cricket',\n",
       "    'athletics',\n",
       "    'football'],\n",
       "   [6.283919080460483,\n",
       "    6.393923673014896,\n",
       "    6.420067859349264,\n",
       "    6.420514184303898,\n",
       "    6.425824789815144,\n",
       "    6.6154703015103316,\n",
       "    6.776325596414108,\n",
       "    6.797738874945792,\n",
       "    6.826742153327064,\n",
       "    6.986706323200539],\n",
       "   [['football'], ['football'], ['football'], ['football'], ['football']]]}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ba18f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'a': 1}, {'b': 2}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {'a': 1}\n",
    "b = {'b':2}\n",
    "l = []\n",
    "l.append(a)\n",
    "l.append(b)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34033527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check 1: Imports done\n",
      "\n",
      "check 2 : function definitions \n",
      "\n",
      " loading train and test dataset\n",
      "\n",
      "check 3 : dataset loaded \n",
      "\n",
      " No of test docs and labels loaded: 222   222 \n",
      "\n",
      " No of train docs and labels loaded: 514   514 \n",
      "\n",
      " Model initialization started wtih NORMAL WMD with MIN cost and glove vectors  \n",
      "\n",
      " check 4: model initialization successful \n",
      "\n",
      "running test4 \n",
      "\n",
      "running test5 \n",
      "running test3 \n",
      "running test6 \n",
      "running test10 \n",
      "running test1 \n",
      "running test2 \n",
      "running test0 \n",
      "running test7 \n",
      "running test11 \n",
      "running test9 \n",
      "running test12 \n",
      "running test8 \n",
      "running test14 \n",
      "running test16 \n",
      "running test15 \n",
      "running test13 \n",
      "\n",
      "\n",
      "\n",
      "running test19 \n",
      "running test18 \n",
      "running test20 \n",
      "\n",
      "\n",
      "\n",
      "running test21 \n",
      "running test22 \n",
      "running test23 \n",
      "running test17 \n",
      "running test24 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Starting distance calculation############################### \n",
      "\n",
      "pred 5 for test 6  ['athletics']\n",
      "pred 7 for test 6  ['athletics']\n",
      "pred 11 for test 6  ['athletics']\n",
      "pred 15 for test 6  ['athletics']\n",
      "pred 21 for test 6  ['athletics']\n",
      "Starting distance calculation############################### \n",
      "\n",
      "pred 5 for test 4  ['athletics']\n",
      "pred 7 for test 4 Starting distance calculation############################### \n",
      "\n",
      " pred 5 for test 0 ['athletics']\n",
      "pred 11 for test 4  ['athletics']\n",
      "pred 7 for test 0  ['athletics'] ['athletics']\n",
      "\n",
      "pred 15 for test 4  pred 11 for test 0 ['athletics'] ['athletics']\n",
      "pred 15 for test 0  \n",
      "['athletics']\n",
      "pred 21 for test 0  ['athletics']pred 21 for test 4 \n",
      " ['athletics']\n",
      "Starting distance calculation############################### \n",
      "\n",
      "Starting distance calculation############################### \n",
      "\n",
      "pred 5 for test 13  ['athletics']pred 5 for test 2  \n",
      "pred 7 for test 13  ['athletics']['athletics']\n",
      "pred 7 for test 2  ['athletics']\n",
      "pred 11 for test 2 \n",
      "pred 11 for test 13  ['athletics'] ['athletics']\n",
      "pred 15 for test 13 \n",
      " pred 15 for test 2  ['athletics']['athletics']\n",
      "\n",
      "pred 21 for test 2 pred 21 for test 13   ['athletics']\n",
      "['athletics']\n",
      "Starting distance calculation############################### \n",
      "\n",
      "pred 5 for test 9  ['athletics']\n",
      "pred 7 for test 9  ['athletics']\n",
      "pred 11 for test 9  ['athletics']\n",
      "pred 15 for test 9  ['athletics']\n",
      "pred 21 for test 9  ['athletics']\n",
      "Starting distance calculation############################### \n",
      "\n",
      "pred 5 for test 17  ['athletics']\n",
      "pred 7 for test 17  ['athletics']\n",
      "pred 11 for test 17  ['athletics']\n",
      "pred 15 for test 17  ['athletics']\n",
      "pred 21 for test 17  ['athletics']\n",
      "Starting distance calculation############################### \n",
      "\n",
      "pred 5 for test 10  ['athletics']\n",
      "pred 7 for test 10  ['athletics']\n",
      "pred 11 for test 10  ['athletics']\n",
      "pred 15 for test 10  ['athletics']\n",
      "pred 21 for test 10  ['athletics']\n",
      "Starting distance calculation############################### \n",
      "\n",
      "pred 5 for test 15  ['athletics']\n",
      "pred 7 for test 15  ['athletics']\n",
      "pred 11 for test 15  ['athletics']\n",
      "pred 15 for test 15  ['athletics']\n",
      "pred 21 for test 15  ['athletics']\n",
      "Starting distance calculation############################### \n",
      "\n",
      "pred 5 for test 8  Starting distance calculation############################### \n",
      "\n",
      "['athletics']\n",
      "pred 7 for test 8  pred 5 for test 20  ['athletics']['athletics']\n",
      "pred 7 for test 20 \n",
      "pred 11 for test 8  ['athletics']\n",
      " ['athletics']pred 11 for test 20 \n",
      " pred 15 for test 8  ['athletics']['athletics']\n",
      "\n",
      "pred 21 for test 8 pred 15 for test 20   ['athletics']['athletics']\n",
      "\n",
      "pred 21 for test 20  ['athletics']\n",
      "Starting distance calculation############################### \n",
      "\n",
      "pred 5 for test 12  ['athletics']\n",
      "pred 7 for test 12  ['athletics']\n",
      "pred 11 for test 12  ['athletics']\n",
      "pred 15 for test 12  ['athletics']\n",
      "pred 21 for test 12  ['athletics']\n",
      "Starting distance calculation############################### \n",
      "\n",
      "pred 5 for test 18  ['athletics']\n",
      "pred 7 for test 18  ['athletics']\n",
      "pred 11 for test 18  ['athletics']\n",
      "pred 15 for test 18  ['athletics']\n",
      "pred 21 for test 18  ['athletics']\n",
      "Starting distance calculation############################### \n",
      "\n",
      "pred 5 for test 1  ['athletics']\n",
      "pred 7 for test 1  ['athletics']\n",
      "pred 11 for test 1  ['athletics']\n",
      "pred 15 for test 1  ['athletics']\n",
      "pred 21 for test 1  ['athletics']\n",
      "Starting distance calculation############################### \n",
      "\n",
      "pred 5 for test 24  ['athletics']\n",
      "pred 7 for test 24  ['athletics']\n",
      "pred 11 for test 24  ['athletics']\n",
      "pred 15 for test 24  ['athletics']\n",
      "pred 21 for test 24  ['athletics']\n",
      "Starting distance calculation############################### \n",
      "\n",
      "pred 5 for test 23  ['athletics']\n",
      "pred 7 for test 23  ['athletics']\n",
      "pred 11 for test 23 Starting distance calculation############################### \n",
      "\n",
      "pred 5 for test 3   ['athletics']['athletics']\n",
      "pred 15 for test 23 \n",
      "pred 7 for test 3  ['athletics']\n",
      " ['athletics']pred 11 for test 3  ['athletics']\n",
      "\n",
      "pred 15 for test 3 pred 21 for test 23  ['athletics'] \n",
      "['athletics']\n",
      "pred 21 for test 3  ['athletics']\n",
      "Starting distance calculation############################### \n",
      "\n",
      "pred 5 for test 11  ['athletics']\n",
      "pred 7 for test 11  ['athletics']\n",
      "pred 11 for test 11  ['athletics']\n",
      "pred 15 for test 11  ['athletics']\n",
      "pred 21 for test 11  ['athletics']\n",
      "Starting distance calculation############################### \n",
      "\n",
      "pred 5 for test 5  ['athletics']\n",
      "pred 7 for test 5  ['athletics']\n",
      "pred 11 for test 5  ['athletics']\n",
      "pred 15 for test 5  ['athletics']\n",
      "pred 21 for test 5  ['athletics']\n",
      "Starting distance calculation############################### \n",
      "\n",
      "pred 5 for test 16  ['athletics']\n",
      "pred 7 for test 16  ['athletics']\n",
      "pred 11 for test 16  ['athletics']\n",
      "pred 15 for test 16  ['athletics']\n",
      "pred 21 for test 16  ['athletics']\n",
      "Starting distance calculation############################### \n",
      "\n",
      "pred 5 for test 22  ['athletics']\n",
      "pred 7 for test 22  ['athletics']\n",
      "pred 11 for test 22  ['athletics']\n",
      "pred 15 for test 22  ['athletics']\n",
      "pred 21 for test 22  ['athletics']\n",
      "Starting distance calculation############################### \n",
      "\n",
      "pred 5 for test 7  ['athletics']\n",
      "pred 7 for test 7  ['athletics']\n",
      "pred 11 for test 7  ['athletics']\n",
      "pred 15 for test 7  ['athletics']\n",
      "pred 21 for test 7  ['athletics']\n",
      "Starting distance calculation############################### \n",
      "\n",
      "pred 5 for test 14  ['athletics']\n",
      "pred 7 for test 14  ['athletics']\n",
      "pred 11 for test 14  ['athletics']\n",
      "pred 15 for test 14  ['athletics']\n",
      "pred 21 for test 14  ['athletics']\n",
      "Starting distance calculation############################### \n",
      "\n",
      "pred 5 for test 21  ['athletics']\n",
      "pred 7 for test 21  ['athletics']\n",
      "pred 11 for test 21  ['athletics']\n",
      "pred 15 for test 21  ['athletics']\n",
      "pred 21 for test 21  ['athletics']\n",
      "Starting distance calculation############################### \n",
      "\n",
      "pred 5 for test 19  ['athletics']\n",
      "pred 7 for test 19  ['athletics']\n",
      "pred 11 for test 19  ['athletics']\n",
      "pred 15 for test 19  ['athletics']\n",
      "pred 21 for test 19  ['athletics']\n",
      "[{0: ['cricket', [1, 3, 2, 0, 4], ['cricket', 'athletics', 'football', 'football', 'athletics'], [6.4483796278225745, 6.663270389922386, 6.83206604978979, 6.947852794854834, 7.062810289714471], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {1: ['tennis', [3, 1, 4, 2, 0], ['athletics', 'cricket', 'athletics', 'football', 'football'], [6.356040006007032, 6.397477020845434, 6.49276396371214, 6.607054366961158, 6.916004403113476], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {2: ['football', [0, 2, 1, 3, 4], ['football', 'football', 'cricket', 'athletics', 'athletics'], [6.665473288590343, 6.666211413750875, 6.741285672838174, 6.7592211501357955, 6.826556391664442], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {3: ['rugby', [3, 2, 1, 0, 4], ['athletics', 'football', 'cricket', 'football', 'athletics'], [6.234518538636698, 6.468995457982604, 6.616508102771298, 6.6743702763363535, 6.768442584095609], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {4: ['tennis', [3, 4, 1, 2, 0], ['athletics', 'athletics', 'cricket', 'football', 'football'], [6.559415515075148, 6.65674648130452, 6.695635677350568, 6.89665714012756, 6.9943506601192285], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {5: ['athletics', [4, 3, 1, 2, 0], ['athletics', 'athletics', 'cricket', 'football', 'football'], [6.55290712840188, 6.559712434515459, 6.8620256743532435, 6.89642461792779, 6.996100808801902], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {6: ['football', [2, 3, 0, 1, 4], ['football', 'athletics', 'football', 'cricket', 'athletics'], [6.922668675327721, 6.980166981499865, 7.197023953131772, 7.235113485654675, 7.365472806229397], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {7: ['football', [2, 1, 3, 0, 4], ['football', 'cricket', 'athletics', 'football', 'athletics'], [6.081091904004365, 6.3127751996194, 6.389353323049086, 6.463785248912359, 6.7079523609450495], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {8: ['football', [3, 2, 4, 0, 1], ['athletics', 'football', 'athletics', 'football', 'cricket'], [7.449627837834137, 7.465358111502782, 7.469529850211532, 7.4865332610350315, 7.5159326767903565], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {9: ['football', [0, 3, 2, 4, 1], ['football', 'athletics', 'football', 'athletics', 'cricket'], [6.151630398033891, 6.2947416187971665, 6.310046900931123, 6.879362753197274, 6.945695481541881], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {10: ['football', [0, 2, 1, 3, 4], ['football', 'football', 'cricket', 'athletics', 'athletics'], [6.785139739392936, 7.041527938498889, 7.106721343304875, 7.1219307580640265, 7.2279141220144645], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {11: ['cricket', [1, 3, 2, 4, 0], ['cricket', 'athletics', 'football', 'athletics', 'football'], [6.34582882677216, 6.925693931336578, 7.001385104602083, 7.162764658544148, 7.400571323021152], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {12: ['tennis', [0, 3, 4, 1, 2], ['football', 'athletics', 'athletics', 'cricket', 'football'], [7.6006231527005195, 7.6117848769710275, 7.697113371933636, 7.716782835397315, 7.780575745962196], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {13: ['football', [0, 2, 3, 1, 4], ['football', 'football', 'athletics', 'cricket', 'athletics'], [6.759662830495171, 6.88941926188718, 6.93555937561521, 7.08364760769283, 7.123040512827055], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {14: ['football', [0, 2, 3, 1, 4], ['football', 'football', 'athletics', 'cricket', 'athletics'], [5.878138614856298, 6.025701846865649, 6.249505398942176, 6.4033391492042995, 6.669249870976194], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {15: ['football', [3, 0, 2, 1, 4], ['athletics', 'football', 'football', 'cricket', 'athletics'], [6.3962543721816445, 6.5094109694183855, 6.594691405981681, 6.828124068793274, 6.924759578611484], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {16: ['football', [2, 3, 1, 4, 0], ['football', 'athletics', 'cricket', 'athletics', 'football'], [6.087857428537418, 6.287886633257836, 6.481919234914751, 6.707629228668902, 6.77586394729441], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {17: ['athletics', [2, 3, 4, 1, 0], ['football', 'athletics', 'athletics', 'cricket', 'football'], [6.265083220937257, 6.365927848768365, 6.384147928650549, 6.726187432580267, 7.457681573573159], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {18: ['cricket', [1, 2, 4, 3, 0], ['cricket', 'football', 'athletics', 'athletics', 'football'], [6.490251720060819, 6.724568562996989, 6.84442121510929, 6.914372769496227, 7.22009671133413], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {19: ['rugby', [3, 2, 1, 4, 0], ['athletics', 'football', 'cricket', 'athletics', 'football'], [6.6154703015103316, 6.776325596414108, 6.797738874945792, 6.826742153327064, 6.986706323200539], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {20: ['football', [2, 0, 3, 1, 4], ['football', 'football', 'athletics', 'cricket', 'athletics'], [6.434244526542779, 6.474829321326844, 6.631573698450989, 6.994995215950736, 7.080443024641795], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {21: ['tennis', [3, 4, 2, 1, 0], ['athletics', 'athletics', 'football', 'cricket', 'football'], [6.772708423992905, 6.7976831011118595, 6.887382448085025, 7.050027626975988, 7.084186305100902], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {22: ['football', [0, 2, 3, 4, 1], ['football', 'football', 'athletics', 'athletics', 'cricket'], [7.198027471017879, 7.21742094451581, 7.338811606848752, 7.423028764732704, 7.433000707355576], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {23: ['cricket', [1, 3, 2, 4, 0], ['cricket', 'athletics', 'football', 'athletics', 'football'], [6.227779076054598, 6.534000281725162, 6.689478788153811, 6.8451966176032375, 7.12654963679228], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {24: ['tennis', [3, 1, 2, 4, 0], ['athletics', 'cricket', 'football', 'athletics', 'football'], [6.504130026388719, 6.705899756998735, 6.711606553732667, 6.854865116172281, 6.91308885044828], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}]\n",
      "\n",
      " time taken:  208.26186418533325\n",
      "..................\n",
      "\n",
      "\n",
      "\n",
      "[{0: ['cricket', [1, 3, 2, 0, 4], ['cricket', 'athletics', 'football', 'football', 'athletics'], [6.4483796278225745, 6.663270389922386, 6.83206604978979, 6.947852794854834, 7.062810289714471], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {1: ['tennis', [3, 1, 4, 2, 0], ['athletics', 'cricket', 'athletics', 'football', 'football'], [6.356040006007032, 6.397477020845434, 6.49276396371214, 6.607054366961158, 6.916004403113476], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {2: ['football', [0, 2, 1, 3, 4], ['football', 'football', 'cricket', 'athletics', 'athletics'], [6.665473288590343, 6.666211413750875, 6.741285672838174, 6.7592211501357955, 6.826556391664442], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {3: ['rugby', [3, 2, 1, 0, 4], ['athletics', 'football', 'cricket', 'football', 'athletics'], [6.234518538636698, 6.468995457982604, 6.616508102771298, 6.6743702763363535, 6.768442584095609], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {4: ['tennis', [3, 4, 1, 2, 0], ['athletics', 'athletics', 'cricket', 'football', 'football'], [6.559415515075148, 6.65674648130452, 6.695635677350568, 6.89665714012756, 6.9943506601192285], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {5: ['athletics', [4, 3, 1, 2, 0], ['athletics', 'athletics', 'cricket', 'football', 'football'], [6.55290712840188, 6.559712434515459, 6.8620256743532435, 6.89642461792779, 6.996100808801902], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {6: ['football', [2, 3, 0, 1, 4], ['football', 'athletics', 'football', 'cricket', 'athletics'], [6.922668675327721, 6.980166981499865, 7.197023953131772, 7.235113485654675, 7.365472806229397], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {7: ['football', [2, 1, 3, 0, 4], ['football', 'cricket', 'athletics', 'football', 'athletics'], [6.081091904004365, 6.3127751996194, 6.389353323049086, 6.463785248912359, 6.7079523609450495], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {8: ['football', [3, 2, 4, 0, 1], ['athletics', 'football', 'athletics', 'football', 'cricket'], [7.449627837834137, 7.465358111502782, 7.469529850211532, 7.4865332610350315, 7.5159326767903565], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {9: ['football', [0, 3, 2, 4, 1], ['football', 'athletics', 'football', 'athletics', 'cricket'], [6.151630398033891, 6.2947416187971665, 6.310046900931123, 6.879362753197274, 6.945695481541881], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {10: ['football', [0, 2, 1, 3, 4], ['football', 'football', 'cricket', 'athletics', 'athletics'], [6.785139739392936, 7.041527938498889, 7.106721343304875, 7.1219307580640265, 7.2279141220144645], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {11: ['cricket', [1, 3, 2, 4, 0], ['cricket', 'athletics', 'football', 'athletics', 'football'], [6.34582882677216, 6.925693931336578, 7.001385104602083, 7.162764658544148, 7.400571323021152], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {12: ['tennis', [0, 3, 4, 1, 2], ['football', 'athletics', 'athletics', 'cricket', 'football'], [7.6006231527005195, 7.6117848769710275, 7.697113371933636, 7.716782835397315, 7.780575745962196], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {13: ['football', [0, 2, 3, 1, 4], ['football', 'football', 'athletics', 'cricket', 'athletics'], [6.759662830495171, 6.88941926188718, 6.93555937561521, 7.08364760769283, 7.123040512827055], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {14: ['football', [0, 2, 3, 1, 4], ['football', 'football', 'athletics', 'cricket', 'athletics'], [5.878138614856298, 6.025701846865649, 6.249505398942176, 6.4033391492042995, 6.669249870976194], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {15: ['football', [3, 0, 2, 1, 4], ['athletics', 'football', 'football', 'cricket', 'athletics'], [6.3962543721816445, 6.5094109694183855, 6.594691405981681, 6.828124068793274, 6.924759578611484], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {16: ['football', [2, 3, 1, 4, 0], ['football', 'athletics', 'cricket', 'athletics', 'football'], [6.087857428537418, 6.287886633257836, 6.481919234914751, 6.707629228668902, 6.77586394729441], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {17: ['athletics', [2, 3, 4, 1, 0], ['football', 'athletics', 'athletics', 'cricket', 'football'], [6.265083220937257, 6.365927848768365, 6.384147928650549, 6.726187432580267, 7.457681573573159], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {18: ['cricket', [1, 2, 4, 3, 0], ['cricket', 'football', 'athletics', 'athletics', 'football'], [6.490251720060819, 6.724568562996989, 6.84442121510929, 6.914372769496227, 7.22009671133413], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {19: ['rugby', [3, 2, 1, 4, 0], ['athletics', 'football', 'cricket', 'athletics', 'football'], [6.6154703015103316, 6.776325596414108, 6.797738874945792, 6.826742153327064, 6.986706323200539], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {20: ['football', [2, 0, 3, 1, 4], ['football', 'football', 'athletics', 'cricket', 'athletics'], [6.434244526542779, 6.474829321326844, 6.631573698450989, 6.994995215950736, 7.080443024641795], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {21: ['tennis', [3, 4, 2, 1, 0], ['athletics', 'athletics', 'football', 'cricket', 'football'], [6.772708423992905, 6.7976831011118595, 6.887382448085025, 7.050027626975988, 7.084186305100902], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {22: ['football', [0, 2, 3, 4, 1], ['football', 'football', 'athletics', 'athletics', 'cricket'], [7.198027471017879, 7.21742094451581, 7.338811606848752, 7.423028764732704, 7.433000707355576], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {23: ['cricket', [1, 3, 2, 4, 0], ['cricket', 'athletics', 'football', 'athletics', 'football'], [6.227779076054598, 6.534000281725162, 6.689478788153811, 6.8451966176032375, 7.12654963679228], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}, {24: ['tennis', [3, 1, 2, 4, 0], ['athletics', 'cricket', 'football', 'athletics', 'football'], [6.504130026388719, 6.705899756998735, 6.711606553732667, 6.854865116172281, 6.91308885044828], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}]\n"
     ]
    }
   ],
   "source": [
    "## To run as batch job\n",
    "\n",
    "\n",
    "#imports:\n",
    "\n",
    "# file imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "import os\n",
    "from scipy.optimize import linprog\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from gensim.models import KeyedVectors\n",
    "import sklearn\n",
    "import scipy\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "print(\"check 1: Imports done\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sentence_preprocess(embed_dict, sentence,lowercase = 1, strip_punctuation = 1,  remove_stopwords = 1,removedigit = 1):\n",
    "    ''' 1 : True, 0 : False : Lowercase, Strip puncutation, Remove Stopwords, removedigit'''\n",
    "\n",
    "    stop_words = list(stopwords.words('english'))\n",
    "\n",
    "    if lowercase == 1:\n",
    "        sentence = sentence.lower()\n",
    "\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "    if strip_punctuation == 1 and removedigit == 1:\n",
    "        sentence_words = [word for word in sentence_words if word.isalpha()] \n",
    "        \n",
    "\n",
    "\n",
    "    if remove_stopwords == 1:\n",
    "        sentence_words = [word for word in sentence_words if not word in stop_words]\n",
    "    \n",
    "    ## to remove those words which are not in the embeddings that we have.\n",
    "    \n",
    "    sentence_words = [word for word in sentence_words if word in embed_dict.keys()]\n",
    "\n",
    "\n",
    "\n",
    "    return sentence_words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "embeddingtype = None\n",
    "embd_model = None\n",
    "\n",
    "\n",
    "\n",
    "## to load from embedding text files:\n",
    "## have used this to load glove vectors and not word2vec\n",
    "\n",
    "def load_glove(embeddingtype):\n",
    "    \n",
    "    if embeddingtype == 3:\n",
    "        i = 300\n",
    "    if embeddingtype == 4:\n",
    "        i = 200\n",
    "    if embeddingtype == 5:\n",
    "        i = 100\n",
    "    if embeddingtype == 6:\n",
    "        i = 50\n",
    "    \n",
    "    \n",
    "    embeddings_dict = defaultdict(lambda:np.zeros(i)) \n",
    "    # defaultdict to take care of OOV words.\n",
    "    \n",
    "    with open(f\"/scratch/Amit_Pandey/wmd_lite/files/glove.6B.{i}d.txt\",'r') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "        \n",
    "    return embeddings_dict\n",
    "\n",
    "\n",
    "def embeddings_setup(newembeddingtype):\n",
    "    \n",
    "    \n",
    "    global embeddingtype\n",
    "    global embd_model\n",
    "    \n",
    "    \n",
    "    '''to avoid loading all the embeddings in the memory.'''\n",
    "    \n",
    "    ''''## Note : we are finding the embd matrix two times, ie once for each sentence in\n",
    "        ## the pair of sentences.\n",
    "        ## so this happens that embedding type is changed when find_embmatrix is called\n",
    "        ## by the first sentence.\n",
    "        The above line doesnt matter now as we not calling find_embmatrix , instead we setting up.\n",
    "    '''\n",
    "        \n",
    "        \n",
    "        \n",
    "    if ( embeddingtype != newembeddingtype):\n",
    "        #print(\"embdtype  entered :\", embeddingtype != newembtype,\"\\n\")\n",
    "        #print(\"embd_model type changed to :\", type(embd_model),\"\\n\" )\n",
    "        \n",
    "        embeddingtype = newembeddingtype\n",
    "        \n",
    "        #embd_model = embeddings_setup(embeddingtype) #adictionary\n",
    "        \n",
    "        #print(\"embd_model type changed to :\", type(embd_model),\"\\n\" )\n",
    "        #to make sure that we don't download the embeddings again and again,\n",
    "        # we will check if the embedding type is same as the old one\n",
    "        # and update global embd_model, vrna next time vo use hi nhi ho payega.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if embeddingtype == 1:\n",
    "        embedding = KeyedVectors.load('google300w2v.kv', mmap='r')\n",
    "        ## This will be slower but will prevent kernel from crashing.\n",
    "        \n",
    "        ## comment the above line and uncomment this if you have sufficient RAM:\n",
    "        \n",
    "        #w2v_emb = gensim.downloader.load('word2vec-google-news-300')\n",
    "        \n",
    "    if embeddingtype == 2:\n",
    "        print('Normalised word2vec not loaded, will get it soon')\n",
    "        embedding = None\n",
    "    \n",
    "    if embeddingtype in (3,4,5,6):\n",
    "        embedding = load_glove(embeddingtype)\n",
    "        \n",
    "    \n",
    "    embd_model = embedding\n",
    "    \n",
    "    \n",
    "        \n",
    "def find_embdMatrix(sentence):\n",
    "    global embeddingtype\n",
    "    global embd_model\n",
    "    #print(\" global embedding type being passed is :\", embeddingtype,\"\\n\")\n",
    "    #print(\"embedding type received by the find emb matrix is :\", newembtype,\"\\n\")\n",
    "    #print(\"embd model type is :\", type(embd_model),\"\\n\")\n",
    "    \n",
    "    sent_mtx = []\n",
    "    \n",
    "    \n",
    "    ##commented lines moved to embedding setup.\n",
    "    \n",
    "#     ''''## Note : we are finding the embd matrix two times, ie once for each sentence in\n",
    "#     ## the pair of sentences.\n",
    "#     ## so this happens that embedding type is changed when find_embmatrix is called\n",
    "#     ## by the first sentence\n",
    "#     '''\n",
    "#     if ( embeddingtype != newembtype):\n",
    "#         print(\"if embdtype part entered :\", embeddingtype != newembtype,\"\\n\")\n",
    "        \n",
    "#         embeddingtype = newembtype\n",
    "#         embd_model = embeddings_setup(embeddingtype) #adictionary\n",
    "        \n",
    "#         print(\"embd_model type changed to :\", type(embd_model),\"\\n\" )\n",
    "#     #to make sure that we don't download the embeddings again and again,\n",
    "#     # we will check if the embedding type is same as the old one\n",
    "#     # and update global embd_model, vrna next time vo use hi nhi ho payega.\n",
    "    \n",
    "    #print(\"embd_model type changed to :\", type(embd_model),\"\\n\" )\n",
    "    for word in sentence:\n",
    "        word_emb = embd_model[word]\n",
    "        sent_mtx.append(word_emb)\n",
    "    \n",
    "    sent_mtx = np.array(sent_mtx).reshape(len(sentence),-1)\n",
    "\n",
    "    return sent_mtx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def wasserstein_distance(pi, qj, D, cost = 'min'):\n",
    "        \"\"\"Find Wasserstein distance through linear programming\n",
    "        p.shape=[m], q.shape=[n], D.shape=[m, n]\n",
    "    \n",
    "        suppose doc1 has m words and doc2 has n words, then an mxn array would be formed, \n",
    "        having distance of each word in doc1 to that of doc2.\n",
    "    \n",
    "    \n",
    "    \n",
    "        p.sum()=1, q.sum()=1, p∈[0,1], q∈[0,1]\n",
    "        \"\"\"\n",
    "        A_eq = [] # a list which will later be converted to array after appending.\n",
    "        for i in range(len(pi)): # len = number of words.\n",
    "            A = np.zeros_like(D) # a 2d array made with the shape of D.  \n",
    "            A[i, :] = 1 \n",
    "            #print(\"Dshape, len pi till here :\",D.shape,len(pi),\"\\n\")\n",
    "            \n",
    "            # to make summation over \"i\" of Tij = pi, ie total / sum of outflow\n",
    "            ## from one word is equal to its pi (normalized bag of word/ frequency/density)\n",
    "            ## ex : if 2x3 D:\n",
    "            ##T1,1 + T1,2 + T1,3 + 0 T2,1 + 0 T2,2 + 0 T2,3 = P1 and so on for every i,\n",
    "            ## ie for each word in the doc1\n",
    "            \n",
    "            \n",
    "            #print(\"A.shape\", A.shape,\"\\n\")\n",
    "            A_eq.append(A.reshape(-1)) ## reshape(-1) flatens and then appending in A_eq.\n",
    "            \n",
    "            #print(A_eq,\"Aeq\\n\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            ## A_eq will be (m+n)x(m.n)\n",
    "    \n",
    "        for i in range(len(qj)):\n",
    "            A = np.zeros_like(D)\n",
    "            A[:, i] = 1 ## summation over \"j\" this time, so this time for different rows, \n",
    "            ## over a column \"j\" which refers to doc2, ie total incoming flow = qj density\n",
    "            A_eq = list(A_eq)\n",
    "            A_eq.append(A.reshape(-1))\n",
    "            A_eq = np.array(A_eq)\n",
    "        \n",
    "        #print(A_eq.shape,A_eq)\n",
    "       \n",
    "        b_eq = np.concatenate([pi, qj])\n",
    "        D = D.reshape(-1)\n",
    "        #print(\"Dshape:\",D.shape)\n",
    "        if cost == 'max':\n",
    "            D = D*(-1)\n",
    "        \n",
    "        result = linprog(D, A_eq=A_eq[:-1], b_eq=b_eq[:-1]) ## removing redundant to make \n",
    "        ## solution more robust.\n",
    "        return np.absolute(result.fun), result.x , D.reshape((len(pi),len(qj)))  ## fun returns the final optimized value, x returns each value of xi,j that is the array\n",
    "\n",
    "    \n",
    "def relaxed_distance(pi,qj,D,cost='min'):\n",
    "    \n",
    "    # to find relaxed we just add the min/max cost directly using the least distance for pi to qj.\n",
    "    \n",
    "    # D is calculated from P to Q ie P in rows and Q in columns, To find Q to P we will transpose \n",
    "    if cost == 'min':\n",
    "        p_to_q = np.dot(D.min(axis=1),pi)\n",
    "        q_to_p = np.dot(D.T.min(axis=1),qj)\n",
    "        \n",
    "        return max(p_to_q,q_to_p)\n",
    "    \n",
    "    if cost == 'max':\n",
    "        \n",
    "        p_to_q = np.dot(D.max(axis=1),pi)\n",
    "        q_to_p = np.dot(D.T.max(axis=1),qj)\n",
    "        \n",
    "        return min(p_to_q,q_to_p), None, D\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "class WMD:\n",
    "    \n",
    "    ''' wmd type = normal/relaxed, costtype = min/max.\n",
    "    Enter Two sentence strings, cost = max if you want to try \n",
    "    max cost max flow version, embeddingtype = 1 for word2vec, 2 = normalized\n",
    "    word2vec, 3 = glove300d, 4 = glove200d, 5 = glove100d 6 = glove50d'''\n",
    "    \n",
    "    def __init__(self,embeddingtype, wmd_type = 'normal', costtype='min'):\n",
    "        \n",
    "        \n",
    "        self.cost = costtype\n",
    "        \n",
    "        self.embeddingtype = embeddingtype \n",
    "        self.wmd_type = wmd_type\n",
    "        \n",
    "        \n",
    "        ## setting up the embeddings\n",
    "        \n",
    "        embeddings_setup(self.embeddingtype)\n",
    "        \n",
    "        self.res = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    #def word_count(self):\n",
    "#         self.sent1_dic = defaultdict(int)\n",
    "#         self.sent2_dic = defaultdict(int)\n",
    "        \n",
    "#         for word in sorted(sentence_preprocess(self.sent1)):\n",
    "#             self.sent1_dic[word] += 1\n",
    "            \n",
    "#         for word in sorted(sentence_preprocess(self.sent2)):\n",
    "#             self.sent2_dic[word] += 1\n",
    "        \n",
    "#         return dict(self.sent1_dic), dict(self.sent2_dic)\n",
    "\n",
    "\n",
    "\n",
    "#     def wasserstein_distance(self, pi, qj, D):\n",
    "#         \"\"\"Find Wasserstein distance through linear programming\n",
    "#         p.shape=[m], q.shape=[n], D.shape=[m, n]\n",
    "    \n",
    "#         suppose doc1 has m words and doc2 has n words, then an mxn array would be formed, \n",
    "#         having distance of each word in doc1 to that of doc2.\n",
    "    \n",
    "    \n",
    "    \n",
    "#         p.sum()=1, q.sum()=1, p∈[0,1], q∈[0,1]\n",
    "#         \"\"\"\n",
    "#         A_eq = [] # a list which will later be converted to array after appending.\n",
    "#         for i in range(len(pi)): # len = number of words.\n",
    "#             A = np.zeros_like(D) # a 2d array made with the shape of D.  \n",
    "#             A[i, :] = 1 \n",
    "#             # to make summation over \"i\" of Tij = pi, ie total / sum of outflow\n",
    "            ## from one word is equal to its pi (normalized bag of word/ frequency/density)\n",
    "            ## ex : if 2x3 D:\n",
    "            ##T1,1 + T1,2 + T1,3 + 0 T2,1 + 0 T2,2 + 0 T2,3 = P1 and so on for every i,\n",
    "            ## ie for each word in the doc1\n",
    "        \n",
    "#             A_eq.append(A.reshape(-1)) ## reshape(-1) flatens and then appending in A_eq.\n",
    "            ## A_eq will be (m+n)x(m.n)\n",
    "    \n",
    "#         for i in range(len(qj)):\n",
    "#             A = np.zeros_like(D)\n",
    "#             A[:, i] = 1 ## summation over \"j\" this time, so this time for different rows, \n",
    "#             ## over a column \"j\" which refers to doc2, ie total incoming flow = qj density\n",
    "#             A_eq.append(A.reshape(-1))\n",
    "#             A_eq = np.array(A_eq)\n",
    "        \n",
    "#         print(A_eq.shape,A_eq)\n",
    "       \n",
    "#         b_eq = np.concatenate([pi, qj])\n",
    "#         D = D.reshape(-1)\n",
    "#         if self.cost == 'max':\n",
    "#             D = D*(-1)\n",
    "        \n",
    "#         result = linprog(D, A_eq=A_eq[:-1], b_eq=b_eq[:-1]) ## removing redundant to make \n",
    "#         ## solution more robust.\n",
    "#         return result.fun, result.x  ## fun returns the final optimized value, x returns each value of xi,j that is the array\n",
    "\n",
    "    \n",
    "    def word_mover_distance(self,sentence1,sentence2):\n",
    "        \n",
    "        self.sent1 = sentence1\n",
    "        #print(self.sent1 ,\"\\n\")\n",
    "        self.sent2 = sentence2\n",
    "        #print(self.sent2 ,\"\\n\")\n",
    "        \n",
    "        \n",
    "        self.sent1_dic = defaultdict(int)\n",
    "        self.sent2_dic = defaultdict(int)\n",
    "        \n",
    "        for word in sorted(sentence_preprocess(embd_model,self.sent1)): # sorted to have better\n",
    "            self.sent1_dic[word] += 1 # idea of the sequence of the words. Creating BOW here\n",
    "            \n",
    "        for word in sorted(sentence_preprocess(embd_model,self.sent2)): #creating BOW from sorted sequence\n",
    "            self.sent2_dic[word] += 1\n",
    "        \n",
    "        \n",
    "        self.sent1_dic = dict(self.sent1_dic) # converted from default dict to dict.\n",
    "        self.sent2_dic = dict(self.sent2_dic) # because following operations work on dict\n",
    "        \n",
    "        \n",
    "        #print(self.sent1_dic ,\"\\n\")\n",
    "        #print(self.sent2_dic ,\"\\n\")\n",
    "        \n",
    "        \n",
    "        ## Now we will store a list/array of all the words in each sentence (in alphabetically sorted order)\n",
    "        ## we will store corresponding count, and then corresponding Normalised count.\n",
    "        self.sent1_words = np.array(list(self.sent1_dic.keys())) #dictionary keys converted to list than array\n",
    "        self.sent1_counts = np.array(list(self.sent1_dic.values()))\n",
    "        \n",
    "        self.sent2_words = np.array(list(self.sent2_dic.keys()))\n",
    "        self.sent2_counts = np.array(list(self.sent2_dic.values()))\n",
    "        \n",
    "        \n",
    "        #print(self.sent1_words ,\"\\n\")\n",
    "        #print(self.sent1_counts ,\"\\n\")\n",
    "        \n",
    "        #print(self.sent2_words ,\"\\n\")\n",
    "        #print(self.sent2_counts ,\"\\n\")\n",
    "        \n",
    "        #dictionary values cant be converted into an array directly, hence the\n",
    "        #list step.\n",
    "        \n",
    "        #print(\"embedding type being passed is :\", self.embeddingtype,\"\\n\")\n",
    "        self.sent1_embmtx = find_embdMatrix(self.sent1_words)\n",
    "        #print(self.sent1_embmtx.shape,\"sent1emb\\n\")\n",
    "        self.sent2_embmtx = find_embdMatrix(self.sent2_words)\n",
    "        #print(self.sent2_embmtx.shape,\"sent2emb\\n\")\n",
    "        \n",
    "        self.pi = self.sent1_counts/np.sum(self.sent1_counts) #NBOW step from BOW\n",
    "        #print(self.pi,\"self.pi\\n\")\n",
    "        self.qj = self.sent2_counts/np.sum(self.sent2_counts)\n",
    "        #print(self.qj,\"self.qj\\n\")\n",
    "        \n",
    "        self.D = np.sqrt(np.square(self.sent1_embmtx[:, None] - self.sent2_embmtx[None, :]).sum(axis=2)) \n",
    "        #print(self.D.shape,\"Dshape \\n\")\n",
    "        ## programmers sought used mean instead of sum.\n",
    "        ## scipy cdist can be used as well.\n",
    "        \n",
    "        if self.wmd_type == 'normal':\n",
    "            return wasserstein_distance(self.pi, self.qj, self.D, self.cost)\n",
    "        \n",
    "        \n",
    "        if self.wmd_type == 'relaxed':\n",
    "            return relaxed_distance(self.pi,self.qj,self.D,self.cost)\n",
    "\n",
    "        \n",
    "\n",
    "print(\"check 2 : function definitions \\n\")\n",
    "\n",
    "## KNN\n",
    "\n",
    "print(\" loading train and test dataset\\n\")\n",
    "\n",
    "Train_BBCsport_sent = np.load(\"/scratch/Amit_Pandey/wmd_lite/files/Train_BBCsport_sent.npy\")\n",
    "Train_BBCsport_label = np.load(\"/scratch/Amit_Pandey/wmd_lite/files/Train_BBCsport_label.npy\")\n",
    "Test_BBCsport_sent = np.load(\"/scratch/Amit_Pandey/wmd_lite/files/Test_BBCsport_sent.npy\")\n",
    "Test_BBCsport_label = np.load(\"/scratch/Amit_Pandey/wmd_lite/files/Test_BBCsport_label.npy\")\n",
    "\n",
    "\n",
    "# for i in range(10):\n",
    "#     print(Test_BBCsport_label[i],\"\\n\",Test_BBCsport_sent[i])\n",
    "    \n",
    "\n",
    "# print(\"##################Train details:\\n\")\n",
    "\n",
    "# for i in range(30):\n",
    "#     print(Train_BBCsport_label[i],\"\\n\",Train_BBCsport_sent[i])\n",
    "\n",
    "\n",
    "# embeddingtype = 3\n",
    "# model = WMD(embeddingtype,wmd_type = 'relaxed', costtype='max')\n",
    "\n",
    "            \n",
    "    \n",
    "no_testdocs = len(Test_BBCsport_sent)\n",
    "no_testlabels = len(Test_BBCsport_label)\n",
    "#no_testdocs,no_testlabels\n",
    "\n",
    "print(\"check 3 : dataset loaded \\n\")\n",
    "\n",
    "print(\" No of test docs and labels loaded:\",no_testdocs,\" \",no_testlabels,\"\\n\")\n",
    "\n",
    "print(\" No of train docs and labels loaded:\",len(Train_BBCsport_sent),\" \",len(Train_BBCsport_label),\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "actual_category = []\n",
    "predicted_category = []\n",
    "prediction_dictionary = {}\n",
    "\n",
    "## prediction_dictionary contains test sentence as key, and [['original lable'],[predicted labels for diff k],\n",
    "## , [index of top labels of train set],['distance of top 30 elements']]\n",
    "\n",
    "    \n",
    "\n",
    "print(\" Model initialization started wtih NORMAL WMD with MIN cost and glove vectors  \\n\")\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "embeddingtype = 3\n",
    "model = WMD(embeddingtype,wmd_type = 'normal', costtype='min')\n",
    "\n",
    "print(\" check 4: model initialization successful \\n\")\n",
    "\n",
    "\n",
    "\n",
    "result = []\n",
    "\n",
    "def predict_Category(i):\n",
    "    \n",
    "    global result \n",
    "    sentence = Test_BBCsport_sent[i]\n",
    "    print(f\"running test{i} \\n\")\n",
    "    #print(\"actual category :\", Test_BBCsport_label[i])\n",
    "    #actual_category.append(Test_BBCsport_label[i])\n",
    "    \n",
    "    distance_fromTrainset = []\n",
    "    \n",
    "    #for j in range (len(Train_BBCsport_sent)):\n",
    "        \n",
    "       \n",
    "    for j in range (5):\n",
    "        #print(f\"Train sent{i} \\n\")\n",
    "        ## Find totalcost ie distance between sentence passed from test set to each sentence \n",
    "        ## in training set. and then append in the list.\n",
    "        \n",
    "        #print(sentence)\n",
    "        #print(Train_BBCsport_sent[i])\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        Totalcost, Tcoeff, Distancematx = model.word_mover_distance(sentence,Train_BBCsport_sent[j])\n",
    "        #print(Totalcost)\n",
    "        distance_fromTrainset.append(Totalcost)\n",
    "        \n",
    "    distance_fromTrainset = np.array(distance_fromTrainset)\n",
    "    #print('distance from train set array:',distance_fromTrainset)\n",
    "    \n",
    "    arr1indx = distance_fromTrainset.argsort()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(\"Original Sentence : \\n\",sentence, \"\\n\",\"Distance and label sorted from test set\\n\",distance_fromTrainset[arr1indx[::1]], \"\\n\",Train_BBCsport_label[arr1indx[::1]],\"\\n\",\" Train Sentences: \\n\",Train_BBCsport_sent[arr1indx[::1]]) \n",
    "    \n",
    "    print(\"Starting distance calculation############################### \\n\")\n",
    "    \n",
    "    ## Taking for different values of K\n",
    "    \n",
    "    #k = 5\n",
    "    sorted_distance_fromTrainset_k5 = distance_fromTrainset[arr1indx[::1]][:5]\n",
    "    sorted_labels_k5 = Train_BBCsport_label[arr1indx[::1]][:5]\n",
    "    \n",
    "    predicted_cat_k5 = scipy.stats.mode(sorted_labels_k5)[0]\n",
    "    print(f\"pred 5 for test {i} \",predicted_cat_k5)\n",
    "   \n",
    "\n",
    "    #k = 7\n",
    "    sorted_distance_fromTrainset_k7 = distance_fromTrainset[arr1indx[::1]][:7]\n",
    "    sorted_labels_k7 = Train_BBCsport_label[arr1indx[::1]][:7]\n",
    "    \n",
    "    predicted_cat_k7 = scipy.stats.mode(sorted_labels_k7)[0]\n",
    "    print(f\"pred 7 for test {i} \",predicted_cat_k7)\n",
    "\n",
    "    #k = 11\n",
    "    sorted_distance_fromTrainset_k11 = distance_fromTrainset[arr1indx[::1]][:11]\n",
    "    sorted_labels_k11 = Train_BBCsport_label[arr1indx[::1]][:11]\n",
    "    \n",
    "    predicted_cat_k11 = scipy.stats.mode(sorted_labels_k11)[0]\n",
    "    print(f\"pred 11 for test {i} \",predicted_cat_k11)\n",
    "\n",
    "    #k = 15\n",
    "    sorted_distance_fromTrainset_k15 = distance_fromTrainset[arr1indx[::1]][:15]\n",
    "    sorted_labels_k15 = Train_BBCsport_label[arr1indx[::1]][:15]\n",
    "    \n",
    "    predicted_cat_k15 = scipy.stats.mode(sorted_labels_k15)[0]\n",
    "    print(f\"pred 15 for test {i} \",predicted_cat_k15)\n",
    "\n",
    "    #k = 21\n",
    "    sorted_distance_fromTrainset_k21 = distance_fromTrainset[arr1indx[::1]][:21]\n",
    "    sorted_labels_k21 = Train_BBCsport_label[arr1indx[::1]][:21]\n",
    "    \n",
    "    predicted_cat_k21 = scipy.stats.mode(sorted_labels_k21)[0]\n",
    "    print(f\"pred 21 for test {i} \",predicted_cat_k21)\n",
    "\n",
    "\n",
    "    #print(sorted_distance_fromTrainset,sorted_labels)\n",
    "\n",
    "    prediction_dictionary[i] = [Test_BBCsport_label[i],\n",
    "                                arr1indx[:30].tolist(),\n",
    "                                Train_BBCsport_label[arr1indx[::1]][:30].tolist(),\n",
    "                                distance_fromTrainset[arr1indx[::1]][:30].tolist(),\n",
    "                                [predicted_cat_k5.tolist(),predicted_cat_k7.tolist(), predicted_cat_k11.tolist(),predicted_cat_k15.tolist(),\n",
    "                                 predicted_cat_k21.tolist()]]\n",
    "    \n",
    "    result.append(prediction_dictionary)\n",
    "    with open('../results/test_result.pickle', 'wb') as handle:\n",
    "        pickle.dump(result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    \n",
    "    return prediction_dictionary\n",
    "\n",
    "    #return np.array([predicted_cat_k5,predicted_cat_k7, predicted_cat_k11,predicted_cat_k15,predicted_cat_k21])#, distance_fromTrainset[arr1indx[::1]],Train_BBCsport_label[arr1indx[::1]]\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "# no_testdocs = len(Test_BBCsport_sent)\n",
    "# no_testlabels = len(Test_BBCsport_label)\n",
    "#no_testdocs,no_testlabels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#predicted_categories_list = []\n",
    "# for i in range (1,2):\n",
    "#     print(Test_BBCsport_label[i])\n",
    "#     actual_categories.append(Test_BBCsport_label[i]) \n",
    "#     pred_category = predict_Category(Test_BBCsport_sent[i])\n",
    "#     print(pred_category)\n",
    "#     predicted_categories_list.append(pred_category)\n",
    "\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "with Pool(25) as p :\n",
    "        predicted_Categorieslist = p.map(predict_Category,range(25))\n",
    "        \n",
    "print(predicted_Categorieslist)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "et = time.time()\n",
    "\n",
    "print(\"\\n time taken: \",et-st)\n",
    "print(\"..................\\n\\n\\n\")\n",
    "print(predicted_Categorieslist)\n",
    "\n",
    "\n",
    "\n",
    "# a_file = open(\"wmdresult.json\", \"w\")\n",
    "# json.dump(predicted_Categorieslist, a_file)\n",
    "# a_file.close()\n",
    "\n",
    "# # a_file = open(\"wmdresult.json\", \"r\")\n",
    "# # output = a_file.read()\n",
    "# # print(output)\n",
    "\n",
    "# # a_file.close()\n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "   \n",
    " \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90fd2245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check 1: Imports done\n",
      "\n",
      "check 2 : function definitions \n",
      "\n",
      " loading train and test dataset\n",
      "\n",
      "check 3 : dataset loaded \n",
      "\n",
      " No of test docs and labels loaded: 222   222 \n",
      "\n",
      " No of train docs and labels loaded: 514   514 \n",
      "\n",
      " Model initialization started wtih NORMAL WMD with MIN cost and glove vectors  \n",
      "\n",
      " check 4: model initialization successful \n",
      "\n",
      "running test0 \n",
      "\n",
      "Starting distance calculation############################### \n",
      "\n",
      "pred 5 for test 0  ['athletics']\n",
      "pred 7 for test 0  ['athletics']\n",
      "pred 11 for test 0  ['athletics']\n",
      "pred 15 for test 0  ['athletics']\n",
      "pred 21 for test 0  ['athletics']\n",
      "[{0: ['cricket', [1, 3, 2, 0, 4], ['cricket', 'athletics', 'football', 'football', 'athletics'], [6.4483796278225745, 6.663270389922386, 6.83206604978979, 6.947852794854834, 7.062810289714471], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}]\n",
      "\n",
      " time taken:  24.162841796875\n",
      "..................\n",
      "\n",
      "\n",
      "\n",
      "[{0: ['cricket', [1, 3, 2, 0, 4], ['cricket', 'athletics', 'football', 'football', 'athletics'], [6.4483796278225745, 6.663270389922386, 6.83206604978979, 6.947852794854834, 7.062810289714471], [['athletics'], ['athletics'], ['athletics'], ['athletics'], ['athletics']]]}]\n"
     ]
    }
   ],
   "source": [
    "## To run as batch job\n",
    "\n",
    "\n",
    "#imports:\n",
    "\n",
    "# file imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "import os\n",
    "from scipy.optimize import linprog\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from gensim.models import KeyedVectors\n",
    "import sklearn\n",
    "import scipy\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "print(\"check 1: Imports done\\n\")\n",
    "\n",
    "def sentence_preprocess(embed_dict, sentence,lowercase = 1, strip_punctuation = 1,  remove_stopwords = 1,removedigit = 1):\n",
    "    ''' 1 : True, 0 : False : Lowercase, Strip puncutation, Remove Stopwords, removedigit'''\n",
    "\n",
    "    stop_words = list(stopwords.words('english'))\n",
    "\n",
    "    if lowercase == 1:\n",
    "        sentence = sentence.lower()\n",
    "\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "    if strip_punctuation == 1 and removedigit == 1:\n",
    "        sentence_words = [word for word in sentence_words if word.isalpha()] \n",
    "        \n",
    "\n",
    "\n",
    "    if remove_stopwords == 1:\n",
    "        sentence_words = [word for word in sentence_words if not word in stop_words]\n",
    "    \n",
    "    ## to remove those words which are not in the embeddings that we have.\n",
    "    \n",
    "    sentence_words = [word for word in sentence_words if word in embed_dict.keys()]\n",
    "\n",
    "\n",
    "\n",
    "    return sentence_words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "embeddingtype = None\n",
    "embd_model = None\n",
    "\n",
    "\n",
    "\n",
    "## to load from embedding text files:\n",
    "## have used this to load glove vectors and not word2vec\n",
    "\n",
    "def load_glove(embeddingtype):\n",
    "    \n",
    "    if embeddingtype == 3:\n",
    "        i = 300\n",
    "    if embeddingtype == 4:\n",
    "        i = 200\n",
    "    if embeddingtype == 5:\n",
    "        i = 100\n",
    "    if embeddingtype == 6:\n",
    "        i = 50\n",
    "    \n",
    "    \n",
    "    embeddings_dict = defaultdict(lambda:np.zeros(i)) \n",
    "    # defaultdict to take care of OOV words.\n",
    "    \n",
    "    with open(f\"/scratch/Amit_Pandey/wmd_lite/files/glove.6B.{i}d.txt\",'r') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "        \n",
    "    return embeddings_dict\n",
    "\n",
    "\n",
    "def embeddings_setup(newembeddingtype):\n",
    "    \n",
    "    \n",
    "    global embeddingtype\n",
    "    global embd_model\n",
    "    \n",
    "    \n",
    "    '''to avoid loading all the embeddings in the memory.'''\n",
    "    \n",
    "    ''''## Note : we are finding the embd matrix two times, ie once for each sentence in\n",
    "        ## the pair of sentences.\n",
    "        ## so this happens that embedding type is changed when find_embmatrix is called\n",
    "        ## by the first sentence.\n",
    "        The above line doesnt matter now as we not calling find_embmatrix , instead we setting up.\n",
    "    '''\n",
    "        \n",
    "        \n",
    "        \n",
    "    if ( embeddingtype != newembeddingtype):\n",
    "        #print(\"embdtype  entered :\", embeddingtype != newembtype,\"\\n\")\n",
    "        #print(\"embd_model type changed to :\", type(embd_model),\"\\n\" )\n",
    "        \n",
    "        embeddingtype = newembeddingtype\n",
    "        \n",
    "        #embd_model = embeddings_setup(embeddingtype) #adictionary\n",
    "        \n",
    "        #print(\"embd_model type changed to :\", type(embd_model),\"\\n\" )\n",
    "        #to make sure that we don't download the embeddings again and again,\n",
    "        # we will check if the embedding type is same as the old one\n",
    "        # and update global embd_model, vrna next time vo use hi nhi ho payega.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if embeddingtype == 1:\n",
    "        embedding = KeyedVectors.load('google300w2v.kv', mmap='r')\n",
    "        ## This will be slower but will prevent kernel from crashing.\n",
    "        \n",
    "        ## comment the above line and uncomment this if you have sufficient RAM:\n",
    "        \n",
    "        #w2v_emb = gensim.downloader.load('word2vec-google-news-300')\n",
    "        \n",
    "    if embeddingtype == 2:\n",
    "        print('Normalised word2vec not loaded, will get it soon')\n",
    "        embedding = None\n",
    "    \n",
    "    if embeddingtype in (3,4,5,6):\n",
    "        embedding = load_glove(embeddingtype)\n",
    "        \n",
    "    \n",
    "    embd_model = embedding\n",
    "    \n",
    "    \n",
    "        \n",
    "def find_embdMatrix(sentence):\n",
    "    global embeddingtype\n",
    "    global embd_model\n",
    "    #print(\" global embedding type being passed is :\", embeddingtype,\"\\n\")\n",
    "    #print(\"embedding type received by the find emb matrix is :\", newembtype,\"\\n\")\n",
    "    #print(\"embd model type is :\", type(embd_model),\"\\n\")\n",
    "    \n",
    "    sent_mtx = []\n",
    "    \n",
    "    \n",
    "    ##commented lines moved to embedding setup.\n",
    "    \n",
    "#     ''''## Note : we are finding the embd matrix two times, ie once for each sentence in\n",
    "#     ## the pair of sentences.\n",
    "#     ## so this happens that embedding type is changed when find_embmatrix is called\n",
    "#     ## by the first sentence\n",
    "#     '''\n",
    "#     if ( embeddingtype != newembtype):\n",
    "#         print(\"if embdtype part entered :\", embeddingtype != newembtype,\"\\n\")\n",
    "        \n",
    "#         embeddingtype = newembtype\n",
    "#         embd_model = embeddings_setup(embeddingtype) #adictionary\n",
    "        \n",
    "#         print(\"embd_model type changed to :\", type(embd_model),\"\\n\" )\n",
    "#     #to make sure that we don't download the embeddings again and again,\n",
    "#     # we will check if the embedding type is same as the old one\n",
    "#     # and update global embd_model, vrna next time vo use hi nhi ho payega.\n",
    "    \n",
    "    #print(\"embd_model type changed to :\", type(embd_model),\"\\n\" )\n",
    "    for word in sentence:\n",
    "        word_emb = embd_model[word]\n",
    "        sent_mtx.append(word_emb)\n",
    "    \n",
    "    sent_mtx = np.array(sent_mtx).reshape(len(sentence),-1)\n",
    "\n",
    "    return sent_mtx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def wasserstein_distance(pi, qj, D, cost = 'min'):\n",
    "        \"\"\"Find Wasserstein distance through linear programming\n",
    "        p.shape=[m], q.shape=[n], D.shape=[m, n]\n",
    "    \n",
    "        suppose doc1 has m words and doc2 has n words, then an mxn array would be formed, \n",
    "        having distance of each word in doc1 to that of doc2.\n",
    "    \n",
    "    \n",
    "    \n",
    "        p.sum()=1, q.sum()=1, p∈[0,1], q∈[0,1]\n",
    "        \"\"\"\n",
    "        A_eq = [] # a list which will later be converted to array after appending.\n",
    "        for i in range(len(pi)): # len = number of words.\n",
    "            A = np.zeros_like(D) # a 2d array made with the shape of D.  \n",
    "            A[i, :] = 1 \n",
    "            #print(\"Dshape, len pi till here :\",D.shape,len(pi),\"\\n\")\n",
    "            \n",
    "            # to make summation over \"i\" of Tij = pi, ie total / sum of outflow\n",
    "            ## from one word is equal to its pi (normalized bag of word/ frequency/density)\n",
    "            ## ex : if 2x3 D:\n",
    "            ##T1,1 + T1,2 + T1,3 + 0 T2,1 + 0 T2,2 + 0 T2,3 = P1 and so on for every i,\n",
    "            ## ie for each word in the doc1\n",
    "            \n",
    "            \n",
    "            #print(\"A.shape\", A.shape,\"\\n\")\n",
    "            A_eq.append(A.reshape(-1)) ## reshape(-1) flatens and then appending in A_eq.\n",
    "            \n",
    "            #print(A_eq,\"Aeq\\n\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            ## A_eq will be (m+n)x(m.n)\n",
    "    \n",
    "        for i in range(len(qj)):\n",
    "            A = np.zeros_like(D)\n",
    "            A[:, i] = 1 ## summation over \"j\" this time, so this time for different rows, \n",
    "            ## over a column \"j\" which refers to doc2, ie total incoming flow = qj density\n",
    "            A_eq = list(A_eq)\n",
    "            A_eq.append(A.reshape(-1))\n",
    "            A_eq = np.array(A_eq)\n",
    "        \n",
    "        #print(A_eq.shape,A_eq)\n",
    "       \n",
    "        b_eq = np.concatenate([pi, qj])\n",
    "        D = D.reshape(-1)\n",
    "        #print(\"Dshape:\",D.shape)\n",
    "        if cost == 'max':\n",
    "            D = D*(-1)\n",
    "        \n",
    "        result = linprog(D, A_eq=A_eq[:-1], b_eq=b_eq[:-1]) ## removing redundant to make \n",
    "        ## solution more robust.\n",
    "        return np.absolute(result.fun), result.x , D.reshape((len(pi),len(qj)))  ## fun returns the final optimized value, x returns each value of xi,j that is the array\n",
    "\n",
    "    \n",
    "def relaxed_distance(pi,qj,D,cost='min'):\n",
    "    \n",
    "    # to find relaxed we just add the min/max cost directly using the least distance for pi to qj.\n",
    "    \n",
    "    # D is calculated from P to Q ie P in rows and Q in columns, To find Q to P we will transpose \n",
    "    if cost == 'min':\n",
    "        p_to_q = np.dot(D.min(axis=1),pi)\n",
    "        q_to_p = np.dot(D.T.min(axis=1),qj)\n",
    "        \n",
    "        return max(p_to_q,q_to_p)\n",
    "    \n",
    "    if cost == 'max':\n",
    "        \n",
    "        p_to_q = np.dot(D.max(axis=1),pi)\n",
    "        q_to_p = np.dot(D.T.max(axis=1),qj)\n",
    "        \n",
    "        return min(p_to_q,q_to_p), None, D\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "class WMD:\n",
    "    \n",
    "    ''' wmd type = normal/relaxed, costtype = min/max.\n",
    "    Enter Two sentence strings, cost = max if you want to try \n",
    "    max cost max flow version, embeddingtype = 1 for word2vec, 2 = normalized\n",
    "    word2vec, 3 = glove300d, 4 = glove200d, 5 = glove100d 6 = glove50d'''\n",
    "    \n",
    "    def __init__(self,embeddingtype, wmd_type = 'normal', costtype='min'):\n",
    "        \n",
    "        \n",
    "        self.cost = costtype\n",
    "        \n",
    "        self.embeddingtype = embeddingtype \n",
    "        self.wmd_type = wmd_type\n",
    "        \n",
    "        \n",
    "        ## setting up the embeddings\n",
    "        \n",
    "        embeddings_setup(self.embeddingtype)\n",
    "        \n",
    "        self.res = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    #def word_count(self):\n",
    "#         self.sent1_dic = defaultdict(int)\n",
    "#         self.sent2_dic = defaultdict(int)\n",
    "        \n",
    "#         for word in sorted(sentence_preprocess(self.sent1)):\n",
    "#             self.sent1_dic[word] += 1\n",
    "            \n",
    "#         for word in sorted(sentence_preprocess(self.sent2)):\n",
    "#             self.sent2_dic[word] += 1\n",
    "        \n",
    "#         return dict(self.sent1_dic), dict(self.sent2_dic)\n",
    "\n",
    "\n",
    "\n",
    "#     def wasserstein_distance(self, pi, qj, D):\n",
    "#         \"\"\"Find Wasserstein distance through linear programming\n",
    "#         p.shape=[m], q.shape=[n], D.shape=[m, n]\n",
    "    \n",
    "#         suppose doc1 has m words and doc2 has n words, then an mxn array would be formed, \n",
    "#         having distance of each word in doc1 to that of doc2.\n",
    "    \n",
    "    \n",
    "    \n",
    "#         p.sum()=1, q.sum()=1, p∈[0,1], q∈[0,1]\n",
    "#         \"\"\"\n",
    "#         A_eq = [] # a list which will later be converted to array after appending.\n",
    "#         for i in range(len(pi)): # len = number of words.\n",
    "#             A = np.zeros_like(D) # a 2d array made with the shape of D.  \n",
    "#             A[i, :] = 1 \n",
    "#             # to make summation over \"i\" of Tij = pi, ie total / sum of outflow\n",
    "            ## from one word is equal to its pi (normalized bag of word/ frequency/density)\n",
    "            ## ex : if 2x3 D:\n",
    "            ##T1,1 + T1,2 + T1,3 + 0 T2,1 + 0 T2,2 + 0 T2,3 = P1 and so on for every i,\n",
    "            ## ie for each word in the doc1\n",
    "        \n",
    "#             A_eq.append(A.reshape(-1)) ## reshape(-1) flatens and then appending in A_eq.\n",
    "            ## A_eq will be (m+n)x(m.n)\n",
    "    \n",
    "#         for i in range(len(qj)):\n",
    "#             A = np.zeros_like(D)\n",
    "#             A[:, i] = 1 ## summation over \"j\" this time, so this time for different rows, \n",
    "#             ## over a column \"j\" which refers to doc2, ie total incoming flow = qj density\n",
    "#             A_eq.append(A.reshape(-1))\n",
    "#             A_eq = np.array(A_eq)\n",
    "        \n",
    "#         print(A_eq.shape,A_eq)\n",
    "       \n",
    "#         b_eq = np.concatenate([pi, qj])\n",
    "#         D = D.reshape(-1)\n",
    "#         if self.cost == 'max':\n",
    "#             D = D*(-1)\n",
    "        \n",
    "#         result = linprog(D, A_eq=A_eq[:-1], b_eq=b_eq[:-1]) ## removing redundant to make \n",
    "#         ## solution more robust.\n",
    "#         return result.fun, result.x  ## fun returns the final optimized value, x returns each value of xi,j that is the array\n",
    "\n",
    "    \n",
    "    def word_mover_distance(self,sentence1,sentence2):\n",
    "        \n",
    "        self.sent1 = sentence1\n",
    "        #print(self.sent1 ,\"\\n\")\n",
    "        self.sent2 = sentence2\n",
    "        #print(self.sent2 ,\"\\n\")\n",
    "        \n",
    "        \n",
    "        self.sent1_dic = defaultdict(int)\n",
    "        self.sent2_dic = defaultdict(int)\n",
    "        \n",
    "        for word in sorted(sentence_preprocess(embd_model,self.sent1)): # sorted to have better\n",
    "            self.sent1_dic[word] += 1 # idea of the sequence of the words. Creating BOW here\n",
    "            \n",
    "        for word in sorted(sentence_preprocess(embd_model,self.sent2)): #creating BOW from sorted sequence\n",
    "            self.sent2_dic[word] += 1\n",
    "        \n",
    "        \n",
    "        self.sent1_dic = dict(self.sent1_dic) # converted from default dict to dict.\n",
    "        self.sent2_dic = dict(self.sent2_dic) # because following operations work on dict\n",
    "        \n",
    "        \n",
    "        #print(self.sent1_dic ,\"\\n\")\n",
    "        #print(self.sent2_dic ,\"\\n\")\n",
    "        \n",
    "        \n",
    "        ## Now we will store a list/array of all the words in each sentence (in alphabetically sorted order)\n",
    "        ## we will store corresponding count, and then corresponding Normalised count.\n",
    "        self.sent1_words = np.array(list(self.sent1_dic.keys())) #dictionary keys converted to list than array\n",
    "        self.sent1_counts = np.array(list(self.sent1_dic.values()))\n",
    "        \n",
    "        self.sent2_words = np.array(list(self.sent2_dic.keys()))\n",
    "        self.sent2_counts = np.array(list(self.sent2_dic.values()))\n",
    "        \n",
    "        \n",
    "        #print(self.sent1_words ,\"\\n\")\n",
    "        #print(self.sent1_counts ,\"\\n\")\n",
    "        \n",
    "        #print(self.sent2_words ,\"\\n\")\n",
    "        #print(self.sent2_counts ,\"\\n\")\n",
    "        \n",
    "        #dictionary values cant be converted into an array directly, hence the\n",
    "        #list step.\n",
    "        \n",
    "        #print(\"embedding type being passed is :\", self.embeddingtype,\"\\n\")\n",
    "        self.sent1_embmtx = find_embdMatrix(self.sent1_words)\n",
    "        #print(self.sent1_embmtx.shape,\"sent1emb\\n\")\n",
    "        self.sent2_embmtx = find_embdMatrix(self.sent2_words)\n",
    "        #print(self.sent2_embmtx.shape,\"sent2emb\\n\")\n",
    "        \n",
    "        self.pi = self.sent1_counts/np.sum(self.sent1_counts) #NBOW step from BOW\n",
    "        #print(self.pi,\"self.pi\\n\")\n",
    "        self.qj = self.sent2_counts/np.sum(self.sent2_counts)\n",
    "        #print(self.qj,\"self.qj\\n\")\n",
    "        \n",
    "        self.D = np.sqrt(np.square(self.sent1_embmtx[:, None] - self.sent2_embmtx[None, :]).sum(axis=2)) \n",
    "        #print(self.D.shape,\"Dshape \\n\")\n",
    "        ## programmers sought used mean instead of sum.\n",
    "        ## scipy cdist can be used as well.\n",
    "        \n",
    "        if self.wmd_type == 'normal':\n",
    "            return wasserstein_distance(self.pi, self.qj, self.D, self.cost)\n",
    "        \n",
    "        \n",
    "        if self.wmd_type == 'relaxed':\n",
    "            return relaxed_distance(self.pi,self.qj,self.D,self.cost)\n",
    "\n",
    "        \n",
    "\n",
    "print(\"check 2 : function definitions \\n\")\n",
    "\n",
    "## KNN\n",
    "\n",
    "print(\" loading train and test dataset\\n\")\n",
    "\n",
    "Train_BBCsport_sent = np.load(\"/scratch/Amit_Pandey/wmd_lite/files/Train_BBCsport_sent.npy\")\n",
    "Train_BBCsport_label = np.load(\"/scratch/Amit_Pandey/wmd_lite/files/Train_BBCsport_label.npy\")\n",
    "Test_BBCsport_sent = np.load(\"/scratch/Amit_Pandey/wmd_lite/files/Test_BBCsport_sent.npy\")\n",
    "Test_BBCsport_label = np.load(\"/scratch/Amit_Pandey/wmd_lite/files/Test_BBCsport_label.npy\")\n",
    "\n",
    "\n",
    "# for i in range(10):\n",
    "#     print(Test_BBCsport_label[i],\"\\n\",Test_BBCsport_sent[i])\n",
    "    \n",
    "\n",
    "# print(\"##################Train details:\\n\")\n",
    "\n",
    "# for i in range(30):\n",
    "#     print(Train_BBCsport_label[i],\"\\n\",Train_BBCsport_sent[i])\n",
    "\n",
    "\n",
    "# embeddingtype = 3\n",
    "# model = WMD(embeddingtype,wmd_type = 'relaxed', costtype='max')\n",
    "\n",
    "            \n",
    "    \n",
    "no_testdocs = len(Test_BBCsport_sent)\n",
    "no_testlabels = len(Test_BBCsport_label)\n",
    "#no_testdocs,no_testlabels\n",
    "\n",
    "print(\"check 3 : dataset loaded \\n\")\n",
    "\n",
    "print(\" No of test docs and labels loaded:\",no_testdocs,\" \",no_testlabels,\"\\n\")\n",
    "\n",
    "print(\" No of train docs and labels loaded:\",len(Train_BBCsport_sent),\" \",len(Train_BBCsport_label),\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "actual_category = []\n",
    "predicted_category = []\n",
    "prediction_dictionary = {}\n",
    "\n",
    "## prediction_dictionary contains test sentence as key, and [['original lable'],[predicted labels for diff k],\n",
    "## , [index of top labels of train set],['distance of top 30 elements']]\n",
    "\n",
    "    \n",
    "\n",
    "print(\" Model initialization started wtih NORMAL WMD with MIN cost and glove vectors  \\n\")\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "embeddingtype = 3\n",
    "model = WMD(embeddingtype,wmd_type = 'normal', costtype='min')\n",
    "\n",
    "print(\" check 4: model initialization successful \\n\")\n",
    "\n",
    "\n",
    "\n",
    "result = []\n",
    "\n",
    "def predict_Category(i):\n",
    "    \n",
    "    global result \n",
    "    sentence = Test_BBCsport_sent[i]\n",
    "    print(f\"running test{i} \\n\")\n",
    "    #print(\"actual category :\", Test_BBCsport_label[i])\n",
    "    #actual_category.append(Test_BBCsport_label[i])\n",
    "    \n",
    "    distance_fromTrainset = []\n",
    "    \n",
    "    #for j in range (len(Train_BBCsport_sent)):\n",
    "        \n",
    "       \n",
    "    for j in range (5):\n",
    "        #print(f\"Train sent{i} \\n\")\n",
    "        ## Find totalcost ie distance between sentence passed from test set to each sentence \n",
    "        ## in training set. and then append in the list.\n",
    "        \n",
    "        #print(sentence)\n",
    "        #print(Train_BBCsport_sent[i])\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        Totalcost, Tcoeff, Distancematx = model.word_mover_distance(sentence,Train_BBCsport_sent[j])\n",
    "        #print(Totalcost)\n",
    "        distance_fromTrainset.append(Totalcost)\n",
    "        \n",
    "    distance_fromTrainset = np.array(distance_fromTrainset)\n",
    "    #print('distance from train set array:',distance_fromTrainset)\n",
    "    \n",
    "    arr1indx = distance_fromTrainset.argsort()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(\"Original Sentence : \\n\",sentence, \"\\n\",\"Distance and label sorted from test set\\n\",distance_fromTrainset[arr1indx[::1]], \"\\n\",Train_BBCsport_label[arr1indx[::1]],\"\\n\",\" Train Sentences: \\n\",Train_BBCsport_sent[arr1indx[::1]]) \n",
    "    \n",
    "    print(\"Starting distance calculation############################### \\n\")\n",
    "    \n",
    "    ## Taking for different values of K\n",
    "    \n",
    "    #k = 5\n",
    "    sorted_distance_fromTrainset_k5 = distance_fromTrainset[arr1indx[::1]][:5]\n",
    "    sorted_labels_k5 = Train_BBCsport_label[arr1indx[::1]][:5]\n",
    "    \n",
    "    predicted_cat_k5 = scipy.stats.mode(sorted_labels_k5)[0]\n",
    "    print(f\"pred 5 for test {i} \",predicted_cat_k5)\n",
    "   \n",
    "\n",
    "    #k = 7\n",
    "    sorted_distance_fromTrainset_k7 = distance_fromTrainset[arr1indx[::1]][:7]\n",
    "    sorted_labels_k7 = Train_BBCsport_label[arr1indx[::1]][:7]\n",
    "    \n",
    "    predicted_cat_k7 = scipy.stats.mode(sorted_labels_k7)[0]\n",
    "    print(f\"pred 7 for test {i} \",predicted_cat_k7)\n",
    "\n",
    "    #k = 11\n",
    "    sorted_distance_fromTrainset_k11 = distance_fromTrainset[arr1indx[::1]][:11]\n",
    "    sorted_labels_k11 = Train_BBCsport_label[arr1indx[::1]][:11]\n",
    "    \n",
    "    predicted_cat_k11 = scipy.stats.mode(sorted_labels_k11)[0]\n",
    "    print(f\"pred 11 for test {i} \",predicted_cat_k11)\n",
    "\n",
    "    #k = 15\n",
    "    sorted_distance_fromTrainset_k15 = distance_fromTrainset[arr1indx[::1]][:15]\n",
    "    sorted_labels_k15 = Train_BBCsport_label[arr1indx[::1]][:15]\n",
    "    \n",
    "    predicted_cat_k15 = scipy.stats.mode(sorted_labels_k15)[0]\n",
    "    print(f\"pred 15 for test {i} \",predicted_cat_k15)\n",
    "\n",
    "    #k = 21\n",
    "    sorted_distance_fromTrainset_k21 = distance_fromTrainset[arr1indx[::1]][:21]\n",
    "    sorted_labels_k21 = Train_BBCsport_label[arr1indx[::1]][:21]\n",
    "    \n",
    "    predicted_cat_k21 = scipy.stats.mode(sorted_labels_k21)[0]\n",
    "    print(f\"pred 21 for test {i} \",predicted_cat_k21)\n",
    "\n",
    "\n",
    "    #print(sorted_distance_fromTrainset,sorted_labels)\n",
    "\n",
    "    prediction_dictionary[i] = [Test_BBCsport_label[i],\n",
    "                                arr1indx[:30].tolist(),\n",
    "                                Train_BBCsport_label[arr1indx[::1]][:30].tolist(),\n",
    "                                distance_fromTrainset[arr1indx[::1]][:30].tolist(),\n",
    "                                [predicted_cat_k5.tolist(),predicted_cat_k7.tolist(), predicted_cat_k11.tolist(),predicted_cat_k15.tolist(),\n",
    "                                 predicted_cat_k21.tolist()]]\n",
    "    \n",
    "    result.append(prediction_dictionary)\n",
    "    with open('../results/test_result.pickle', 'wb') as handle:\n",
    "        pickle.dump(result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    \n",
    "    return prediction_dictionary\n",
    "\n",
    "    #return np.array([predicted_cat_k5,predicted_cat_k7, predicted_cat_k11,predicted_cat_k15,predicted_cat_k21])#, distance_fromTrainset[arr1indx[::1]],Train_BBCsport_label[arr1indx[::1]]\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "# no_testdocs = len(Test_BBCsport_sent)\n",
    "# no_testlabels = len(Test_BBCsport_label)\n",
    "#no_testdocs,no_testlabels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#predicted_categories_list = []\n",
    "# for i in range (1,2):\n",
    "#     print(Test_BBCsport_label[i])\n",
    "#     actual_categories.append(Test_BBCsport_label[i]) \n",
    "#     pred_category = predict_Category(Test_BBCsport_sent[i])\n",
    "#     print(pred_category)\n",
    "#     predicted_categories_list.append(pred_category)\n",
    "\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "with Pool(1) as p :\n",
    "        predicted_Categorieslist = p.map(predict_Category,range(1))\n",
    "        \n",
    "print(predicted_Categorieslist)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "et = time.time()\n",
    "\n",
    "print(\"\\n time taken: \",et-st)\n",
    "print(\"..................\\n\\n\\n\")\n",
    "print(predicted_Categorieslist)\n",
    "\n",
    "\n",
    "\n",
    "# a_file = open(\"wmdresult.json\", \"w\")\n",
    "# json.dump(predicted_Categorieslist, a_file)\n",
    "# a_file.close()\n",
    "\n",
    "# # a_file = open(\"wmdresult.json\", \"r\")\n",
    "# # output = a_file.read()\n",
    "# # print(output)\n",
    "\n",
    "# # a_file.close()\n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "   \n",
    " \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0ae4114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{19: ['rugby',\n",
       "   [3, 2, 1, 4, 0],\n",
       "   ['athletics', 'football', 'cricket', 'athletics', 'football'],\n",
       "   [6.6154703015103316,\n",
       "    6.776325596414108,\n",
       "    6.797738874945792,\n",
       "    6.826742153327064,\n",
       "    6.986706323200539],\n",
       "   [['athletics'],\n",
       "    ['athletics'],\n",
       "    ['athletics'],\n",
       "    ['athletics'],\n",
       "    ['athletics']]]}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../results/test_result.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c0879b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
